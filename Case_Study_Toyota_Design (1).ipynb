{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38b0be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from Helper_Functions import z_expectation_variance,moment_matching_update,product_diff_list,question_extractor\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pylogit as pl\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from choicedesign.design import EffDesign\n",
    "#from biogeme.expressions import Beta, Variable\n",
    "#import biogeme.database as db\n",
    "#import biogeme.models as models\n",
    "\n",
    "#import pyDOE2\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd915a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5962c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function simulates a decision maker picking between objects x and y.\n",
    "def simulated_decision_maker_selection(true_partworth,x,y):\n",
    "    #true_partworth: this is a numpy array of partworths of the decision maker\n",
    "    #x and y: two objects for the decision maker to compare, should be numpy arrays. \n",
    "    \n",
    "    preferred_option = x\n",
    "    not_preferred = y\n",
    "    epsilon_y = rng.gumbel(loc = 0.0, scale = 1.0)\n",
    "    epsilon_x = rng.gumbel(loc = 0.0, scale = 1.0)\n",
    "    \n",
    "    \n",
    "    if np.dot(true_partworth,y) + epsilon_y > np.dot(true_partworth,x) + epsilon_x:\n",
    "        preferred_option = y\n",
    "        not_preferred = x\n",
    "    \n",
    "    return preferred_option,not_preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eed040f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>choice_situation</th>\n",
       "      <th>alt_id</th>\n",
       "      <th>choice</th>\n",
       "      <th>id</th>\n",
       "      <th>choice_id</th>\n",
       "      <th>Color_Metallic</th>\n",
       "      <th>Color_White</th>\n",
       "      <th>Color_Gray</th>\n",
       "      <th>Wheels_Factory2</th>\n",
       "      <th>Wheels_Factory3</th>\n",
       "      <th>FB_Factory2</th>\n",
       "      <th>FB_Factory3</th>\n",
       "      <th>RB_Custom</th>\n",
       "      <th>HL_Factory2</th>\n",
       "      <th>TL_Factory2</th>\n",
       "      <th>SM_Custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   choice_situation  alt_id  choice  id  choice_id  Color_Metallic  \\\n",
       "0                 1       1     NaN   1          1               0   \n",
       "1                 1       2     NaN   1          1               0   \n",
       "2                 2       1     NaN   1          2               0   \n",
       "3                 2       2     NaN   1          2               1   \n",
       "4                 3       1     NaN   1          3               0   \n",
       "\n",
       "   Color_White  Color_Gray  Wheels_Factory2  Wheels_Factory3  FB_Factory2  \\\n",
       "0            0           0                1                0            0   \n",
       "1            0           1                0                0            1   \n",
       "2            1           0                1                0            0   \n",
       "3            0           0                0                1            0   \n",
       "4            0           1                1                0            1   \n",
       "\n",
       "   FB_Factory3  RB_Custom  HL_Factory2  TL_Factory2  SM_Custom  \n",
       "0            1          0            0            0          0  \n",
       "1            0          1            1            1          1  \n",
       "2            1          1            0            1          1  \n",
       "3            0          0            1            0          0  \n",
       "4            0          0            1            0          1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import a choice design for the simulated decision maker to participate in. This will be used to construct a prior\n",
    "prior_design_data = pd.read_csv('Toyota_Corolla_Prior_Design.csv', header = 0)\n",
    "prior_design_data.head() # to display the first 5 lines of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b95f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      choice_situation  alt_id  choice  id  choice_id  Color_Metallic  \\\n",
      "2035              1018       2     1.0  85         10               0   \n",
      "2036              1019       1     1.0  85         11               0   \n",
      "2037              1019       2     0.0  85         11               0   \n",
      "2038              1020       1     1.0  85         12               0   \n",
      "2039              1020       2     0.0  85         12               0   \n",
      "\n",
      "      Color_White  Color_Gray  Wheels_Factory2  Wheels_Factory3  FB_Factory2  \\\n",
      "2035            1           0                0                1            0   \n",
      "2036            0           1                0                0            0   \n",
      "2037            1           0                1                0            1   \n",
      "2038            1           0                0                0            0   \n",
      "2039            0           0                1                0            0   \n",
      "\n",
      "      FB_Factory3  RB_Custom  HL_Factory2  TL_Factory2  SM_Custom  \n",
      "2035            1          1            1            0          0  \n",
      "2036            1          1            0            0          1  \n",
      "2037            0          0            1            1          0  \n",
      "2038            1          0            0            1          0  \n",
      "2039            0          1            1            0          1  \n"
     ]
    }
   ],
   "source": [
    "#Here, we simulate a group of individuals participating in a static questionnaire and we will end up using the\n",
    "#estimates of the model parameters from this study as prior information for our decision maker of interest.\n",
    "\n",
    "#Specify group mean and variance\n",
    "group_pref_mean = np.array([1.0,-0.5,-0.25,0.0,0.0,0.5,-0.5,-0.5,0.0,0.0,0.0])\n",
    "#group_pref_var = np.array([[0.25, 0.125, -0.0625, 0,0,0,0,0,0,0,0],\n",
    "                           #[0.125, 0.25, -0.125, 0,0,0,0,0,0,0,0],\n",
    "                           #[-0.0625, -0.125, 0.25, 0,0,0,0,0,0,0,0],\n",
    "                           #[0,0,0, 0.25, -0.125, 0,0,0,0,0,0],\n",
    "                           #[0,0,0, -0.125, 0.25, 0,0,0,0,0,0],\n",
    "                           #[0,0,0,0,0, 0.25, -0.125,0,0,0,0],\n",
    "                           #[0,0,0,0,0, -0.125, 0.25,0,0,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0.25,0,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0.25,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0,0.25,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0,0,0.25]])\n",
    "group_pref_var = 1.0*np.identity(11)\n",
    "\n",
    "for i in range(85):\n",
    "    subject_i_partworth = rng.multivariate_normal(group_pref_mean,group_pref_var)\n",
    "    for j in range(12):\n",
    "        x_j = prior_design_data.iloc[2*j + 24*i,5:16].to_numpy()\n",
    "        y_j = prior_design_data.iloc[2*j + 24*i + 1, 5:16].to_numpy()\n",
    "        #print(x_j)\n",
    "        #print(y_j)\n",
    "        pref_i = simulated_decision_maker_selection(subject_i_partworth,x_j,y_j)[0]\n",
    "        #print(pref_i)\n",
    "        if np.array_equal(x_j,pref_i):\n",
    "            prior_design_data.iloc[2*j + 24*i,2] = int(1)\n",
    "            prior_design_data.iloc[2*j + 24*i + 1, 2] = int(0)\n",
    "        else:\n",
    "            prior_design_data.iloc[2*j + 24*i,2] = int(0)\n",
    "            prior_design_data.iloc[2*j + 24*i + 1, 2] = int(1)\n",
    "\n",
    "        \n",
    "print(prior_design_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8a8251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wsfishe\\Anaconda3\\lib\\site-packages\\pylogit\\estimation.py:678: RuntimeWarning: Method BFGS does not use Hessian information (hess).\n",
      "  results = minimize(estimator.calc_neg_log_likelihood_and_neg_gradient,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.68860826 -0.39757709 -0.182961    0.13940557  0.04375253  0.69371932\n",
      " -0.41265107 -0.56369929  0.02419032 -0.00909917  0.21836482 -0.71708677\n",
      "  0.35236927 -0.55350302  0.9148141  -1.04892393  0.6970341  -0.64998374\n",
      "  0.97304123  1.13156931 -0.71867921 -0.83703548]\n",
      "[ 0.68860826 -0.39757709 -0.182961    0.13940557  0.04375253  0.69371932\n",
      " -0.41265107 -0.56369929  0.02419032 -0.00909917  0.21836482]\n",
      "[[0.51421343 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.1241641  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.30636559 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.83688485 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         1.10024141 0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.48585654\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.42247887 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.94680924 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.28044909 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.5164998  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.7006284 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create the model specification for the screening study\n",
    "v_spec = OrderedDict()\n",
    "v_names = OrderedDict()\n",
    "\n",
    "for col, display_name in [(\"Color_Metallic\", \"Color_Metallic\"),(\"Color_White\",\"Color_White\"),(\"Color_Gray\",\"Color_Gray\"),\n",
    "                         (\"Wheels_Factory2\",\"Wheels_Factory2\"),(\"Wheels_Factory3\",\"Wheels_Factory3\"),\n",
    "                         (\"FB_Factory2\",\"FB_Factory2\"),(\"FB_Factory3\",\"FB_Factory3\"),(\"RB_Custom\",\"RB_Custom\"),(\"HL_Factory2\",\"HL_Factory2\"),\n",
    "                         (\"TL_Factory2\",\"TL_Factory2\"),(\"SM_Custom\",\"SM_Custom\")]:\n",
    "    v_spec[col] = [[1,2]]\n",
    "    v_names[col] = [display_name]\n",
    "\n",
    "    \n",
    "# List the variables that are the index variables (NEW 8/14/2024)\n",
    "index_var_names = [\"Color_Metallic\", \"Color_White\", \"Color_Gray\", \"Wheels_Factory2\", \"Wheels_Factory3\", \"FB_Factory2\",\n",
    "                  \"FB_Factory3\",\"RB_Custom\",\"HL_Factory2\",\"TL_Factory2\",\"SM_Custom\"]\n",
    "\n",
    "# Transform all of the index variable columns to have float dtypes\n",
    "for col in index_var_names:\n",
    "    prior_design_data[col] = prior_design_data[col].astype(float)\n",
    "\n",
    "#Test this out.\n",
    "#prior_design_data_limited = prior_design_data[prior_design_data[\"id\"] <= 10]\n",
    "\n",
    "#COMMENT THIS OUT 8/14/2024\n",
    "#v_model = pl.create_choice_model(data = prior_design_data, #prior_design_data_limited,#test this out.\n",
    "                                #alt_id_col = \"alt_id\",\n",
    "                                #obs_id_col = \"choice_situation\",\n",
    "                                #choice_col = \"choice\",\n",
    "                                #specification = v_spec,\n",
    "                                #model_type = \"MNL\",\n",
    "                                #names = v_names,\n",
    "                                #)\n",
    "\n",
    "v_model_mixed = pl.create_choice_model(data = prior_design_data,\n",
    "                                      alt_id_col = \"alt_id\",\n",
    "                                      obs_id_col = \"choice_situation\",\n",
    "                                      choice_col = \"choice\",\n",
    "                                      specification = v_spec,\n",
    "                                      model_type = \"Mixed Logit\",\n",
    "                                      names = v_names,\n",
    "                                      mixing_id_col = \"id\",\n",
    "                                      mixing_vars = index_var_names)\n",
    "\n",
    "#COMMENT THIS OUT 8/14/2024\n",
    "#We will save the utility coefficient estimates and variance.\n",
    "#v_model_fit = v_model.fit_mle(np.zeros(11), just_point = True)\n",
    "\n",
    "v_model_mixed_fit = v_model_mixed.fit_mle(init_vals=np.zeros(2 * len(index_var_names)),\n",
    "                      num_draws=600,\n",
    "                      seed=123, just_point = True)\n",
    "\n",
    "# Look at the estimated results\n",
    "#v_model_mixed.get_statsmodels_summary()\n",
    "\n",
    "#MAY CHANGE 8/14/2024\n",
    "#Save partworths and covariance matrix. We scale the covariance by 85 since we had 85 participants.\n",
    "#v_model_partworths = np.array(v_model_fit[\"x\"])\n",
    "#v_model_covariance = 85*np.array(v_model_fit[\"hess_inv\"])\n",
    "\n",
    "v_model_mixed_estimates = np.array(v_model_mixed_fit[\"x\"])\n",
    "\n",
    "#print('v_model_partworths: ' + str(v_model_partworths))\n",
    "#print('v_model_covariance: ' + str(v_model_covariance))\n",
    "#print('v_model_covariance scaled by number of participants: ' + str(85.0 * v_model_covariance))\n",
    "\n",
    "print(v_model_mixed_estimates)\n",
    "\n",
    "v_model_partworths = v_model_mixed_estimates[0:11]\n",
    "v_model_covariance = np.diag(np.square(v_model_mixed_estimates[11:22]))\n",
    "\n",
    "print(v_model_partworths)\n",
    "print(v_model_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aef6b937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood at zero: -707.0101\n",
      "Initial Log-likelihood: -707.0101\n",
      "Estimation Time for Point Estimation: 15.49 seconds.\n",
      "Final log-likelihood: -632.4926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wsfishe\\Anaconda3\\lib\\site-packages\\pylogit\\base_multinomial_cm_v2.py:1259: RuntimeWarning: invalid value encountered in sqrt\n",
      "  self._store_inferential_results(np.sqrt(np.diag(self.robust_cov)),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Mixed Logit Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>choice</td>       <th>  No. Observations:  </th>   <td>1,020</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>         <td>Mixed Logit Model</td> <th>  Df Residuals:      </th>    <td>998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>        <th>  Df Model:          </th>    <td>22</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Wed, 11 Sep 2024</td>  <th>  Pseudo R-squ.:     </th>   <td>0.105</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>10:16:13</td>      <th>  Pseudo R-bar-squ.: </th>   <td>0.074</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>AIC:</th>               <td>1,308.985</td>     <th>  Log-Likelihood:    </th> <td>-632.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>BIC:</th>               <td>1,417.391</td>     <th>  LL-Null:           </th> <td>-707.010</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Color_Metallic</th>        <td>    0.6886</td> <td>    0.254</td> <td>    2.708</td> <td> 0.007</td> <td>    0.190</td> <td>    1.187</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Color_White</th>           <td>   -0.3976</td> <td>    0.193</td> <td>   -2.060</td> <td> 0.039</td> <td>   -0.776</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Color_Gray</th>            <td>   -0.1830</td> <td>    0.228</td> <td>   -0.802</td> <td> 0.423</td> <td>   -0.630</td> <td>    0.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Wheels_Factory2</th>       <td>    0.1394</td> <td>    0.172</td> <td>    0.811</td> <td> 0.417</td> <td>   -0.197</td> <td>    0.476</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Wheels_Factory3</th>       <td>    0.0438</td> <td>    0.230</td> <td>    0.190</td> <td> 0.849</td> <td>   -0.407</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FB_Factory2</th>           <td>    0.6937</td> <td>    0.242</td> <td>    2.869</td> <td> 0.004</td> <td>    0.220</td> <td>    1.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>FB_Factory3</th>           <td>   -0.4127</td> <td>    0.165</td> <td>   -2.506</td> <td> 0.012</td> <td>   -0.735</td> <td>   -0.090</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RB_Custom</th>             <td>   -0.5637</td> <td>    0.150</td> <td>   -3.764</td> <td> 0.000</td> <td>   -0.857</td> <td>   -0.270</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>HL_Factory2</th>           <td>    0.0242</td> <td>    0.177</td> <td>    0.137</td> <td> 0.891</td> <td>   -0.322</td> <td>    0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TL_Factory2</th>           <td>   -0.0091</td> <td>    0.133</td> <td>   -0.068</td> <td> 0.946</td> <td>   -0.271</td> <td>    0.253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SM_Custom</th>             <td>    0.2184</td> <td>    0.141</td> <td>    1.552</td> <td> 0.121</td> <td>   -0.057</td> <td>    0.494</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma Color_Metallic</th>  <td>   -0.7171</td> <td>    0.292</td> <td>   -2.454</td> <td> 0.014</td> <td>   -1.290</td> <td>   -0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma Color_White</th>     <td>    0.3524</td> <td>    0.313</td> <td>    1.126</td> <td> 0.260</td> <td>   -0.261</td> <td>    0.966</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma Color_Gray</th>      <td>   -0.5535</td> <td>    0.371</td> <td>   -1.490</td> <td> 0.136</td> <td>   -1.281</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma Wheels_Factory2</th> <td>    0.9148</td> <td>    0.233</td> <td>    3.924</td> <td> 0.000</td> <td>    0.458</td> <td>    1.372</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma Wheels_Factory3</th> <td>   -1.0489</td> <td>    0.329</td> <td>   -3.190</td> <td> 0.001</td> <td>   -1.693</td> <td>   -0.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma FB_Factory2</th>     <td>    0.6970</td> <td>    0.290</td> <td>    2.404</td> <td> 0.016</td> <td>    0.129</td> <td>    1.265</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma FB_Factory3</th>     <td>   -0.6500</td> <td>    0.247</td> <td>   -2.636</td> <td> 0.008</td> <td>   -1.133</td> <td>   -0.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma RB_Custom</th>       <td>    0.9730</td> <td>    0.236</td> <td>    4.116</td> <td> 0.000</td> <td>    0.510</td> <td>    1.436</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma HL_Factory2</th>     <td>    1.1316</td> <td>    0.247</td> <td>    4.588</td> <td> 0.000</td> <td>    0.648</td> <td>    1.615</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma TL_Factory2</th>     <td>   -0.7187</td> <td>    0.200</td> <td>   -3.595</td> <td> 0.000</td> <td>   -1.111</td> <td>   -0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sigma SM_Custom</th>       <td>   -0.8370</td> <td>    0.181</td> <td>   -4.623</td> <td> 0.000</td> <td>   -1.192</td> <td>   -0.482</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &       choice      & \\textbf{  No. Observations:  } &   1,020     \\\\\n",
       "\\textbf{Model:}                 & Mixed Logit Model & \\textbf{  Df Residuals:      } &    998      \\\\\n",
       "\\textbf{Method:}                &        MLE        & \\textbf{  Df Model:          } &     22      \\\\\n",
       "\\textbf{Date:}                  &  Wed, 11 Sep 2024 & \\textbf{  Pseudo R-squ.:     } &   0.105     \\\\\n",
       "\\textbf{Time:}                  &      10:16:13     & \\textbf{  Pseudo R-bar-squ.: } &   0.074     \\\\\n",
       "\\textbf{AIC:}                   &     1,308.985     & \\textbf{  Log-Likelihood:    } &  -632.493   \\\\\n",
       "\\textbf{BIC:}                   &     1,417.391     & \\textbf{  LL-Null:           } &  -707.010   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Color\\_Metallic}        &       0.6886  &        0.254     &     2.708  &         0.007        &        0.190    &        1.187     \\\\\n",
       "\\textbf{Color\\_White}           &      -0.3976  &        0.193     &    -2.060  &         0.039        &       -0.776    &       -0.019     \\\\\n",
       "\\textbf{Color\\_Gray}            &      -0.1830  &        0.228     &    -0.802  &         0.423        &       -0.630    &        0.264     \\\\\n",
       "\\textbf{Wheels\\_Factory2}       &       0.1394  &        0.172     &     0.811  &         0.417        &       -0.197    &        0.476     \\\\\n",
       "\\textbf{Wheels\\_Factory3}       &       0.0438  &        0.230     &     0.190  &         0.849        &       -0.407    &        0.494     \\\\\n",
       "\\textbf{FB\\_Factory2}           &       0.6937  &        0.242     &     2.869  &         0.004        &        0.220    &        1.168     \\\\\n",
       "\\textbf{FB\\_Factory3}           &      -0.4127  &        0.165     &    -2.506  &         0.012        &       -0.735    &       -0.090     \\\\\n",
       "\\textbf{RB\\_Custom}             &      -0.5637  &        0.150     &    -3.764  &         0.000        &       -0.857    &       -0.270     \\\\\n",
       "\\textbf{HL\\_Factory2}           &       0.0242  &        0.177     &     0.137  &         0.891        &       -0.322    &        0.370     \\\\\n",
       "\\textbf{TL\\_Factory2}           &      -0.0091  &        0.133     &    -0.068  &         0.946        &       -0.271    &        0.253     \\\\\n",
       "\\textbf{SM\\_Custom}             &       0.2184  &        0.141     &     1.552  &         0.121        &       -0.057    &        0.494     \\\\\n",
       "\\textbf{Sigma Color\\_Metallic}  &      -0.7171  &        0.292     &    -2.454  &         0.014        &       -1.290    &       -0.144     \\\\\n",
       "\\textbf{Sigma Color\\_White}     &       0.3524  &        0.313     &     1.126  &         0.260        &       -0.261    &        0.966     \\\\\n",
       "\\textbf{Sigma Color\\_Gray}      &      -0.5535  &        0.371     &    -1.490  &         0.136        &       -1.281    &        0.174     \\\\\n",
       "\\textbf{Sigma Wheels\\_Factory2} &       0.9148  &        0.233     &     3.924  &         0.000        &        0.458    &        1.372     \\\\\n",
       "\\textbf{Sigma Wheels\\_Factory3} &      -1.0489  &        0.329     &    -3.190  &         0.001        &       -1.693    &       -0.405     \\\\\n",
       "\\textbf{Sigma FB\\_Factory2}     &       0.6970  &        0.290     &     2.404  &         0.016        &        0.129    &        1.265     \\\\\n",
       "\\textbf{Sigma FB\\_Factory3}     &      -0.6500  &        0.247     &    -2.636  &         0.008        &       -1.133    &       -0.167     \\\\\n",
       "\\textbf{Sigma RB\\_Custom}       &       0.9730  &        0.236     &     4.116  &         0.000        &        0.510    &        1.436     \\\\\n",
       "\\textbf{Sigma HL\\_Factory2}     &       1.1316  &        0.247     &     4.588  &         0.000        &        0.648    &        1.615     \\\\\n",
       "\\textbf{Sigma TL\\_Factory2}     &      -0.7187  &        0.200     &    -3.595  &         0.000        &       -1.111    &       -0.327     \\\\\n",
       "\\textbf{Sigma SM\\_Custom}       &      -0.8370  &        0.181     &    -4.623  &         0.000        &       -1.192    &       -0.482     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Mixed Logit Model Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                     Mixed Logit Model Regression Results                     \n",
       "==============================================================================\n",
       "Dep. Variable:                 choice   No. Observations:                1,020\n",
       "Model:              Mixed Logit Model   Df Residuals:                      998\n",
       "Method:                           MLE   Df Model:                           22\n",
       "Date:                Wed, 11 Sep 2024   Pseudo R-squ.:                   0.105\n",
       "Time:                        10:16:13   Pseudo R-bar-squ.:               0.074\n",
       "AIC:                        1,308.985   Log-Likelihood:               -632.493\n",
       "BIC:                        1,417.391   LL-Null:                      -707.010\n",
       "=========================================================================================\n",
       "                            coef    std err          z      P>|z|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------------\n",
       "Color_Metallic            0.6886      0.254      2.708      0.007       0.190       1.187\n",
       "Color_White              -0.3976      0.193     -2.060      0.039      -0.776      -0.019\n",
       "Color_Gray               -0.1830      0.228     -0.802      0.423      -0.630       0.264\n",
       "Wheels_Factory2           0.1394      0.172      0.811      0.417      -0.197       0.476\n",
       "Wheels_Factory3           0.0438      0.230      0.190      0.849      -0.407       0.494\n",
       "FB_Factory2               0.6937      0.242      2.869      0.004       0.220       1.168\n",
       "FB_Factory3              -0.4127      0.165     -2.506      0.012      -0.735      -0.090\n",
       "RB_Custom                -0.5637      0.150     -3.764      0.000      -0.857      -0.270\n",
       "HL_Factory2               0.0242      0.177      0.137      0.891      -0.322       0.370\n",
       "TL_Factory2              -0.0091      0.133     -0.068      0.946      -0.271       0.253\n",
       "SM_Custom                 0.2184      0.141      1.552      0.121      -0.057       0.494\n",
       "Sigma Color_Metallic     -0.7171      0.292     -2.454      0.014      -1.290      -0.144\n",
       "Sigma Color_White         0.3524      0.313      1.126      0.260      -0.261       0.966\n",
       "Sigma Color_Gray         -0.5535      0.371     -1.490      0.136      -1.281       0.174\n",
       "Sigma Wheels_Factory2     0.9148      0.233      3.924      0.000       0.458       1.372\n",
       "Sigma Wheels_Factory3    -1.0489      0.329     -3.190      0.001      -1.693      -0.405\n",
       "Sigma FB_Factory2         0.6970      0.290      2.404      0.016       0.129       1.265\n",
       "Sigma FB_Factory3        -0.6500      0.247     -2.636      0.008      -1.133      -0.167\n",
       "Sigma RB_Custom           0.9730      0.236      4.116      0.000       0.510       1.436\n",
       "Sigma HL_Factory2         1.1316      0.247      4.588      0.000       0.648       1.615\n",
       "Sigma TL_Factory2        -0.7187      0.200     -3.595      0.000      -1.111      -0.327\n",
       "Sigma SM_Custom          -0.8370      0.181     -4.623      0.000      -1.192      -0.482\n",
       "=========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot the summary just to check the model.\n",
    "v_model_mixed.fit_mle(init_vals=np.zeros(2 * len(index_var_names)),\n",
    "                      num_draws=600,\n",
    "                      seed=123)\n",
    "v_model_mixed.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ad5ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a list of difference in products which satisfy dummy coding constraints for a specified list of \n",
    "#levels\n",
    "def product_diff_list_casestudy(levels):\n",
    "    ##levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    \n",
    "    #calculate the number of binary variables in the attribute vector.\n",
    "    num_binary_var = sum([lev - 1 for lev in levels])\n",
    "    \n",
    "    #make a call to product_diff_list to get a larger set of questions (difference between two alternatives) which violates the dummy coding constraints. We\n",
    "    #will filter out questions which violate the constraints.\n",
    "    question_list = product_diff_list(num_binary_var)\n",
    "    \n",
    "    #Create a list of questions which satisfy the dummy coding constraints\n",
    "    filtered_question_list = []\n",
    "    \n",
    "    \n",
    "    num_levels = len(levels)\n",
    "    \n",
    "    #For each question, we check whether it violates the dummy coding. If not, it gets added to our list of \n",
    "    #filtered questions.\n",
    "    for question in question_list:\n",
    "        lower_bound = 0\n",
    "        upper_bound = levels[0]-1\n",
    "        num_violations = 0\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            abs_sum_lev_question = sum([abs(question[j]) for j in range(lower_bound, upper_bound)])\n",
    "            if abs_sum_lev_question > 2:\n",
    "                num_violations = num_violations + 1\n",
    "            #Update lower and upper bound to go to the next level.\n",
    "            lower_bound = upper_bound\n",
    "            if i < num_levels-1:\n",
    "                upper_bound = upper_bound + (levels[i+1]-1)\n",
    "        \n",
    "        if num_violations == 0:\n",
    "            filtered_question_list.append(question)\n",
    "    \n",
    "    #The filtering above leaves questions which can have two levels of a factor activated at once. For example,\n",
    "    #for a 3x2 design the above filtering leaves the question [1 1 -1], which is not possible under dummy coding.\n",
    "    #We must filter out questions whose levels sum exactly to 2 (and also -2).\n",
    "    further_filtered_question_list = []\n",
    "    \n",
    "    for question in filtered_question_list:\n",
    "        lower_bound = 0\n",
    "        upper_bound = levels[0]-1\n",
    "        num_violations = 0\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            sum_lev_question = sum([question[j] for j in range(lower_bound, upper_bound)])\n",
    "            if sum_lev_question == 2 or sum_lev_question == -2:\n",
    "                num_violations = num_violations + 1\n",
    "            #Update lower and upper bound to go to the next level.\n",
    "            lower_bound = upper_bound\n",
    "            if i < num_levels-1:\n",
    "                upper_bound = upper_bound + (levels[i+1]-1)\n",
    "        \n",
    "        if num_violations == 0:\n",
    "            further_filtered_question_list.append(question)\n",
    "    \n",
    "    return further_filtered_question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the product_diff_list_casestudy function above.\n",
    "#print(product_diff_list_casestudy([3,3,2,2,2,2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8578a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to generate data to estimate the parameters in the normalized AO model. The normalized AO model\n",
    "#is given by log(D-err/Det^(1/2)(Sig)) ~ AM/||L*mu|| + AV/||S*Sig|| + AO/||S*Sig|| + ||L*mu|| + ||S*Sig||. AM, AV, and AO denote\n",
    "#the average question mean, average quesiton variance, and average question orthogonality of a given design under prior\n",
    "#N(mu, Sig). L and S denote varying signal and noise levels, respectively. The normalized AO model is used in our\n",
    "#optimization procedure so that we will not have to refit the parameters in the optimization model everytime the user\n",
    "#answers a batch. The idea is that varying L and S enough should encompass a wide enough range so that mu and Sig will\n",
    "#be within this range after updating.\n",
    "#In this function, we also include MO so that we may fit a maximum orthogonality model.\n",
    "\n",
    "#!!! rng will need to be set before calling this function !!!\n",
    "\n",
    "def norm_AO_MO_data_generation_casestudy(init_mu, init_Sig, batch_size, L, S, num_random_batches, num_true_partworths,rng,levels):\n",
    "    #init_mu: This is the initial expectation of the partworths. Should be a numpy array.\n",
    "    \n",
    "    #init_Sig: This is the initial covariance matrix of the partworths. Should be a square two-dimensional numpy array\n",
    "    #having rows and columns with same number of entries corresponding to init_mu.\n",
    "    \n",
    "    #batch_size: This is the number of questions in each batch. Should be an integer greater than or equal to one.\n",
    "    \n",
    "    #L: This is a vector which holds varying levels of signal (multiply with mu). For example,\n",
    "    #we could have L = [0.25,1.0,4.0]\n",
    "    \n",
    "    #S: This is a vector which holds varying levels of noise (multiply with Sig). For example,\n",
    "    #we could have S = [0.25,1.0,4.0]\n",
    "    \n",
    "    #num_random_batches: This is the number of random batches that we will generate for collecting data on log(D-err),\n",
    "    #AM, AV, and AO (and MO). This set of random batches will be used for each level combination of L and S. Should be an integer\n",
    "    #greater than or equal to one.\n",
    "    \n",
    "    #num_true_partworths: This is the number of true/baseline partworths we will use to evaluate the d-error of a design. Should be\n",
    "    #an integer greater than or equal to one.\n",
    "    \n",
    "    #rng: random number generator for generating multivariate normal vectors. Should be of the form rng = np.random.default_rng(seed)\n",
    "    #for some seed value.\n",
    "    \n",
    "    #levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    \n",
    "    attr_num = len(init_mu)\n",
    "    \n",
    "    #Create lists to store average orthogonality and max orthogonality, as well as d-error and average question mean and\n",
    "    #average question variance, and ||L*mu|| and ||S*Sig|| as well.\n",
    "    average_orthogonality = []\n",
    "    \n",
    "    maximum_orthogonality = []\n",
    "    \n",
    "    average_question_mean = []\n",
    "    \n",
    "    average_question_variance = []\n",
    "    \n",
    "    average_d_error = []\n",
    "    \n",
    "    L_mu = []\n",
    "    \n",
    "    S_Sig = []\n",
    "    \n",
    "    init_sqrt_determinant = []\n",
    "    \n",
    "    #Create a list of all products. WE USE product_diff_list_casestudy here instead of product_diff_list.\n",
    "    #prod_list = product_diff_list(attr_num)\n",
    "    prod_list = product_diff_list_casestudy(levels)\n",
    "    \n",
    "    #Construct the set of batch designs\n",
    "    batch_set = [[] for i in range(num_random_batches)]\n",
    "    for i in range(num_random_batches):\n",
    "        random_question_matrix = random.sample(prod_list,batch_size)\n",
    "        for m in range(batch_size):\n",
    "            #random_question_vec = random.sample(prod_list,1)[0]\n",
    "            #[x,y] = question_extractor(random_question_vec)\n",
    "            [x,y] = question_extractor(random_question_matrix[m])\n",
    "            batch_set[i].append([x,y])\n",
    "    \n",
    "    #Record the scaled norm of mu and Sig for each combination of L and S\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                L_mu.append(l*np.linalg.norm(init_mu,2))\n",
    "                S_Sig.append(s*np.linalg.norm(init_Sig,2))\n",
    "                init_sqrt_determinant.append(np.sqrt(np.linalg.det(s*init_Sig)))\n",
    "    \n",
    "    #Calculate AM, AV, AO and MO for each of the batches\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                random_batch_question_mean = []\n",
    "                random_batch_question_variance = []\n",
    "                random_batch_orthogonality = []\n",
    "        \n",
    "                for p in range(batch_size):\n",
    "                    x_p = np.array(batch_set[i][p][0])\n",
    "                    y_p = np.array(batch_set[i][p][1])\n",
    "                    random_batch_question_mean.append(np.abs(np.dot(l*init_mu,x_p - y_p)))\n",
    "                    random_batch_question_variance.append(np.dot(x_p - y_p, np.dot(s*init_Sig,x_p - y_p)))\n",
    "                    for q in range(p+1, batch_size):\n",
    "                        x_q = np.array(batch_set[i][q][0])\n",
    "                        y_q = np.array(batch_set[i][q][1])\n",
    "                        random_batch_orthogonality.append(np.abs(np.dot(x_p - y_p, np.dot(s*init_Sig,x_q - y_q))))\n",
    "                \n",
    "                #We use this if statement in case the batch size is 1 because\n",
    "                #if the batch size is 1 then there are no orthogonality terms.\n",
    "                if len(random_batch_orthogonality) > 0:\n",
    "                    average_orthogonality.append(np.mean(np.array(random_batch_orthogonality)))\n",
    "                    maximum_orthogonality.append(np.max(np.array(random_batch_orthogonality)))\n",
    "        \n",
    "                average_question_mean.append(np.mean(np.array(random_batch_question_mean)))\n",
    "                average_question_variance.append(np.mean(np.array(random_batch_question_variance)))\n",
    "            \n",
    "    #Calculate the D-error.\n",
    "    for l in L:\n",
    "        print('L: '+str(l))\n",
    "        for s in S:\n",
    "            print('S: '+str(s))\n",
    "            true_partworths = []\n",
    "            for t in range(num_true_partworths):\n",
    "                #This is where rng needs to be set beforehand!\n",
    "                true_partworths.append(rng.multivariate_normal(l*init_mu,s*init_Sig))\n",
    "                \n",
    "            gumbel_errors = [[[np.random.gumbel(0,1) for k in range(2)] for j in range(batch_size)] for i in range(num_true_partworths)]\n",
    "            \n",
    "            for i in range(num_random_batches):\n",
    "                #Create a list for the batch that will store the final determinant value for each simulation\n",
    "                #corresponding to each baseline partworth.\n",
    "                batch_simulate_d_values = []\n",
    "                \n",
    "                #Simulate d-efficiency over baseline partworths\n",
    "                for j in range(len(true_partworths)):\n",
    "                #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "                    mu = l*init_mu\n",
    "                    Sig = s*init_Sig\n",
    "                    \n",
    "                    #Each simulation goes through the questions in the random batch.\n",
    "                    for k in range(batch_size):\n",
    "                    #Set x and y\n",
    "                        x = batch_set[i][k][0]\n",
    "                        y = batch_set[i][k][1]\n",
    "                \n",
    "                        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "                        x_temp = x\n",
    "                        y_temp = y\n",
    "                        \n",
    "                        gum_x = gumbel_errors[j][k][0]\n",
    "                        gum_y = gumbel_errors[j][k][1]\n",
    "                        #See preference between two products\n",
    "                        if (np.dot(true_partworths[j],np.array(y)) + gum_y) >= (np.dot(true_partworths[j],np.array(x)) + gum_x):\n",
    "                            x = y_temp\n",
    "                            y = x_temp\n",
    "                            \n",
    "                        #Perform moment matching after choice is made.\n",
    "                        [mu, Sig] = moment_matching_update(x,y,mu,Sig)\n",
    "                        \n",
    "                    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "                    #of the final covariance matrix.\n",
    "                    batch_simulate_d_values.append(np.sqrt(np.linalg.det(Sig)))\n",
    "                    \n",
    "                #We average the d-values from the simulation for a batch and store it in a list. This is the D-error of the batch i\n",
    "                #under distribution N(L*mu, S*Sig).\n",
    "                average_d_error.append(np.mean(batch_simulate_d_values))\n",
    "                \n",
    "    return average_orthogonality, maximum_orthogonality, average_question_mean, average_question_variance, L_mu, S_Sig, init_sqrt_determinant, average_d_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a443f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function constructs a batch design based off of average question mean, average question variance, and average\n",
    "#question orthogonality. For the average question orthogonality, we take the absolute value of the summands rather than\n",
    "#the square. We also normalize mu and Sig in the objective so that we do not need to keep on refitting the parameters \n",
    "#that go with question mean, question variance, and question orthogonality.\n",
    "\n",
    "#For the case study, we add functionality which allows constraints to enforce dummy coding for attributes which have more \n",
    "#than two levels.\n",
    "\n",
    "def batch_design_AO_casestudy(mu,Sig,batch_size,quest_mean_log_coeff,quest_var_log_coeff,quest_orth_log_coeff,t_lim = 100,logfile=False, levels = []):\n",
    "    #mu: expectation of prior on the DM's partworth. Should be a numpy array.\n",
    "    \n",
    "    #Sig: Covariance matrix of prior on the DM's partworth.  Should be a square two-dimensional numpy array\n",
    "    #having rows and columns with same number of entries corresponding to mu.\n",
    "    \n",
    "    \n",
    "    #batch_size: the number of questions we want to return in our batch design. This should be less or equal to the number\n",
    "    #of attributes (length of mu).\n",
    "    \n",
    "    #quest_mean_log_coeff: this is a fitting parameter that goes with the average question mean and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AM/||l*mu||.\n",
    "    \n",
    "    #quest_var_log_coeff: this is a fitting parameter that goes with the average question variance and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AV/||s*Sig||.\n",
    "    \n",
    "    #quest_orth_log_coeff: this is a fitting parameter that goes with the average question orthogonality and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AO/||s*Sig||.\n",
    "    \n",
    "    #In the above three comments regarding the coefficients, (l,s) are scaling parameters for mu and Sig that divide the space into \n",
    "    #different signal-to-noise ratio regions.\n",
    "    \n",
    "    #t_lim: this is the max amount of time we want to take to construct the batch\n",
    "    #logfile: determine whether to print out a logfile of the optimization procedure.\n",
    "    \n",
    "    #levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    #This will be used to enforce dummy coding constraints when there are more than two levels. The default value is []. In\n",
    "    #the default case, it is assumed all attributes have two levels. It should be the case that (outside the default\n",
    "    #case) the sum of (each entry\n",
    "    #in levels - 1) should equal the length of mu, due to the dummy coding conventions.\n",
    "\n",
    "    #Make sure that quest_orth_log_coeff is greater or equal to zero. Otherwise, we will\n",
    "    #have an unbounded optimization problem. In most situations, the fitting procedure\n",
    "    #will result in a positive value for quest_orth_log_coeff, but very rarely the fitting\n",
    "    #procedure will give a statistically non-significant but negative value for\n",
    "    #quest_orth_log_coeff that makes the optimization problem unbounded. When the quest_orth_log_coeff\n",
    "    #is less than 0, we decide to set it equal to 0. This will result in a bounded optimization problem,\n",
    "    #but the quality of the solution in terms of D-error may not be sufficient because we are no\n",
    "    #longer controlling orthogonality in the objective function.\n",
    "    if quest_orth_log_coeff<0.0:\n",
    "        quest_orth_log_coeff = 0.0\n",
    "\n",
    "    # This is the number of attributes for the products\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('Timelimit',t_lim)\n",
    "    if logfile:\n",
    "        m.setParam('LogFile',\"Batch_AO_batchsize\"+str(batch_size)+\"_meancoeff_\"+str(quest_mean_log_coeff)+\"_varcoeff_\"+\n",
    "               str(quest_var_log_coeff)+\"_orthcoeff_\"+str(quest_orth_log_coeff)+\"_v5.txt\")\n",
    "    \n",
    "    #calculate 2-norms of mu and Sigma\n",
    "    mu_2norm = np.linalg.norm(mu,2)\n",
    "    Sig_2norm = np.linalg.norm(Sig,2)\n",
    "    \n",
    "    #List of tuples for delta variable\n",
    "    if batch_size > 1:\n",
    "        delta_tuples = []\n",
    "        for i in range(batch_size):\n",
    "            for j in range(i+1,batch_size):\n",
    "                delta_tuples.append((i,j))\n",
    "    \n",
    "    #Set up the x_i and y_i, i = 1,...,batchsize\n",
    "    X = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    Y = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    if batch_size > 1:\n",
    "        Delta = m.addVars(delta_tuples, lb=0.0, vtype = GRB.CONTINUOUS)\n",
    "    \n",
    "    #Set up the objective function.\n",
    "    if batch_size > 1:\n",
    "        m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) + \n",
    "                       (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                       Y[i]@Sig@Y[i] for i in range(batch_size)]) + \n",
    "                           (quest_orth_log_coeff/(batch_size*(batch_size-1)*Sig_2norm/2))*sum([Delta[i,j] for i in range(batch_size) for j in range(i+1,batch_size)]),GRB.MINIMIZE)\n",
    "        \n",
    "    if batch_size == 1:\n",
    "        m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) + \n",
    "                       (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                       Y[i]@Sig@Y[i] for i in range(batch_size)]),GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up the constraints that force the products in question i to be different, as well as forcing the symmetry\n",
    "    #exploitation condition.\n",
    "    for i in range(batch_size):\n",
    "        m.addConstr(X[i]@X[i] - X[i]@Y[i] - Y[i]@X[i] + Y[i]@Y[i] >= 1)\n",
    "        m.addConstr(mu@X[i] - mu@Y[i] >= 0)\n",
    "        \n",
    "    #Set up the Sigma-orthogonality constraint for all questions i and j, i not equal to j. Also add constraints\n",
    "    #to make sure that questions within a batch are different, including with respect to switching order of products in\n",
    "    #the questions.\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1,batch_size):\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] - Delta[i,j] <= 0)\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] + Delta[i,j] >= 0)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@X[j] + X[i]@Y[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@X[j] - Y[i]@Y[j] -\n",
    "                       X[j]@X[i] + X[j]@Y[i] + X[j]@X[j] - X[j]@Y[j] +\n",
    "                       Y[j]@X[i] - Y[j]@Y[i] - Y[j]@X[j] + Y[j]@Y[j] >= 1)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@Y[j] + X[i]@X[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@Y[j] - Y[i]@X[j] -\n",
    "                       Y[j]@X[i] + Y[j]@Y[i] + Y[j]@Y[j] - Y[j]@X[j] +\n",
    "                       X[j]@X[i] - X[j]@Y[i] - X[j]@Y[j] + X[j]@X[j] >= 1)\n",
    "            \n",
    "    #We add the dummy coding constraints if there are attributes with more than two levels. (7/27/2024)\n",
    "    num_attributes = len(levels)\n",
    "    if num_attributes > 0:\n",
    "        for i in range(batch_size):\n",
    "            index_level_tracker = 0\n",
    "            for j in range(num_attributes):\n",
    "                m.addConstr(sum([X[i,k] for k in range(index_level_tracker,index_level_tracker + levels[j]-1)]) <= 1)\n",
    "                m.addConstr(sum([Y[i,k] for k in range(index_level_tracker,index_level_tracker + levels[j]-1)]) <= 1)\n",
    "                index_level_tracker = index_level_tracker + (levels[j]-1)\n",
    "                #print(index_level_tracker)\n",
    "                \n",
    "            \n",
    "    m.optimize()\n",
    "    \n",
    "    #This will be the list of products\n",
    "    Q = [ [] for i in range(batch_size)]\n",
    "    D = [ [] for i in range(batch_size-1)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        Q[i].append(X[i].X)\n",
    "        Q[i].append(Y[i].X)\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1, batch_size):\n",
    "            D[i].append(Delta[i,j].X)\n",
    "        \n",
    "    return[Q,D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93133b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the batch_design_AO_casestudy function above.\n",
    "#mu_test = np.ones(7)\n",
    "#Sig_test = np.identity(7)\n",
    "#batch_test = 4\n",
    "#mean_coeff_test = 0.03\n",
    "#var_coeff_test = -0.01\n",
    "#orth_coeff_test = 0.005\n",
    "#levels_test = [4,3,2,2]\n",
    "#print(batch_design_AO_casestudy(mu_test,Sig_test,batch_test,mean_coeff_test,var_coeff_test,orth_coeff_test,levels = levels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3aa41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_dm_partworth: [ 0.67866979 -0.98566148  1.43005813  1.97052688  0.15518366 -0.75776545\n",
      " -0.29978158 -0.45794701 -0.87801647  1.5294079   0.99553717]\n"
     ]
    }
   ],
   "source": [
    "#Create our targeted decision maker.\n",
    "rng2 = np.random.default_rng(1000)\n",
    "target_dm_partworth = rng2.multivariate_normal(group_pref_mean,group_pref_var)\n",
    "print('target_dm_partworth: ' + str(target_dm_partworth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we start the process of learning the parameters for the MIP-AC (batch_design_AO_casestudy) model.\n",
    "rng3 = np.random.default_rng(101)\n",
    "\n",
    "\n",
    "batch_size_fit = 5\n",
    "\n",
    "#These are the vectors L and S discussed in \"Offline Learning Framework for Specifying MIP Objective Parameters\"\n",
    "L_fit = [0.5,1.0,2.0]\n",
    "S_fit = [0.5,1.0,2.0]\n",
    "\n",
    "#num_random_batches_fit is the number of batches for which we evaluate D-error, average question mean,\n",
    "#average question variance, average & maximum question covariance (orthogonality). num_true_partworths_fit is \n",
    "#the number of partworths used in simulating/evaluating D-error of each design.\n",
    "num_random_batches_fit = 1000\n",
    "num_true_partworths_fit = 50\n",
    "\n",
    "v_levels = [4,3,3,2,2,2,2]\n",
    "\n",
    "#Generate the data in order to estimate the parameters of the AO and MO models\n",
    "average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit = norm_AO_MO_data_generation_casestudy(v_model_partworths, v_model_covariance, batch_size_fit, L_fit, S_fit, num_random_batches_fit, num_true_partworths_fit,rng3,v_levels)\n",
    "\n",
    "#Create a dataframe of the generated data for fitting the parameters of the AO(MIP-AC) and MO(MIP-MC) models\n",
    "df_fit = pd.DataFrame(list(zip(average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit)),\n",
    "                  columns =['Avg_Orth', 'Max_Orth', 'Avg_Quest_Mean', 'Avg_Quest_Var', 'L_mu_norm', 'S_Sig_norm', 'Init_Sqrt_Det', 'D_err'])\n",
    "\n",
    "#Add some new columns to the dataset. We mean-center the independent variables to attempt to reduce VIF. This will not affect the value of\n",
    "#of the coefficients, except for the intercept. The intercept is not important because it is constant and thus will not be used\n",
    "#in the optimization problem.\n",
    "df_fit['log_norm_derr'] = np.log(np.divide(np.array(df_fit['D_err']),np.array(df_fit['Init_Sqrt_Det'])))\n",
    "df_fit['cent_norm_AM'] = np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])))\n",
    "df_fit['cent_norm_AV'] = np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_AO'] = np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_MO'] = np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "\n",
    "df_fit['cent_L_mu_norm'] = df_fit['L_mu_norm'] - np.mean(np.array(df_fit['L_mu_norm']))\n",
    "df_fit['cent_S_Sig_norm'] = df_fit['S_Sig_norm'] - np.mean(np.array(df_fit['S_Sig_norm']))\n",
    "\n",
    "#Fitting the linear model for the AO (MIP-AC) model. Note that we do not use batch size\n",
    "#because it will be constant throughout the questionnaire.\n",
    "model_AO = sm.formula.ols(formula = \"log_norm_derr ~  cent_norm_AM + cent_norm_AV + cent_norm_AO + cent_L_mu_norm + cent_S_Sig_norm\", data = df_fit).fit()\n",
    "parameter_est_AO = model_AO.params\n",
    "\n",
    "print('parameter_est_AO: ' + str(parameter_est_AO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the questionnaire for the targeted decision maker.\n",
    "\n",
    "number_of_questions_target = 20\n",
    "batch_size_target = 5\n",
    "\n",
    "#Set the optimization model parameter estimates for MIP-AC\n",
    "AO_alpha_exp = parameter_est_AO[1]\n",
    "AO_kappa_exp = parameter_est_AO[2]\n",
    "AO_gamma_exp = parameter_est_AO[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f85e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the questionnaire\n",
    "mu_target = v_model_partworths\n",
    "Sig_target = v_model_covariance\n",
    "\n",
    "for j in range(number_of_questions_target):\n",
    "    \n",
    "    if (j % batch_size_target == 0):\n",
    "        batch_AO = batch_design_AO_casestudy(mu_target,Sig_target,batch_size_target,AO_alpha_exp,AO_kappa_exp,AO_gamma_exp,t_lim = 50,levels = v_levels)[0]\n",
    "        print(batch_AO)\n",
    "\n",
    "    [x_target,y_target] = batch_AO[j % batch_size_target]\n",
    "    \n",
    "    [pref,not_pref] = simulated_decision_maker_selection(target_dm_partworth,x_target,y_target)\n",
    "    print([pref,not_pref])\n",
    "        \n",
    "    #Perform moment matching after choice is made.\n",
    "    [mu_target, Sig_target] = moment_matching_update(pref,not_pref,mu_target,Sig_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe covariance and estimates of partworths after finishing the questionnaire. We observe the covariance in order\n",
    "#to get standard errors of the partworths.\n",
    "print('mu_target: ' + str(mu_target))\n",
    "print('Sig_target: ' + str(Sig_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fccfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare estimated ranking of top 5 vehicles for true partworth and estimated partworth\n",
    "\n",
    "#Create a new dataframe which has all the vehicle configurations. This dataframe comes from the dataset\n",
    "#Toyota_Corolla_Concept_Designs\n",
    "\n",
    "vehicle_concept_designs = pd.read_csv(\"Toyota_Corolla_Concept_Designs.csv\",header = 0)\n",
    "\n",
    "#We start by adding two columns to the dataframe, which are the utilities of each vehicle configuration under\n",
    "#the true partworth, and utilities under the estimated partworth.\n",
    "\n",
    "true_mean_utility = []\n",
    "est_mean_utility = []\n",
    "\n",
    "for i in range(len(vehicle_concept_designs)):\n",
    "    vehicle_i = vehicle_concept_designs.iloc[i,1:13].to_numpy()\n",
    "    #print(vehicle_i)\n",
    "    true_mean_utility.append(np.dot(target_dm_partworth,vehicle_i))\n",
    "    est_mean_utility.append(np.dot(mu_target,vehicle_i))\n",
    "\n",
    "print('true_mean_utility: ' + str(true_mean_utility))\n",
    "print('est_mean_utility: ' + str(est_mean_utility))\n",
    "\n",
    "vehicle_concept_designs[\"true_mean_util\"] = true_mean_utility\n",
    "vehicle_concept_designs[\"est_mean_util\"] = est_mean_utility\n",
    "\n",
    "vehicle_concept_designs_true_util = vehicle_concept_designs.sort_values(by = 'true_mean_util',ascending = False)\n",
    "\n",
    "print(vehicle_concept_designs_true_util.head())\n",
    "\n",
    "vehicle_concept_designs_est_util = vehicle_concept_designs.sort_values(by = 'est_mean_util', ascending = False)\n",
    "\n",
    "print(vehicle_concept_designs_est_util.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate standard deviations and CI's\n",
    "standard_deviations_final = np.sqrt(np.diag(Sig_target))\n",
    "print(\"sd: \" + str(standard_deviations_final))\n",
    "\n",
    "lower_CI = mu_target - 1.96*standard_deviations_final\n",
    "upper_CI = mu_target + 1.96*standard_deviations_final\n",
    "\n",
    "print(\"Low CI: \" + str(lower_CI))\n",
    "print(\"Up CI: \" + str(upper_CI))\n",
    "\n",
    "cover = []\n",
    "for i in range(11):\n",
    "    if lower_CI[i] <= target_dm_partworth[i] and upper_CI[i] >= target_dm_partworth[i]:\n",
    "        cover.append(\"Cover\")\n",
    "    else:\n",
    "        cover.append(\"Not Cover\")\n",
    "        \n",
    "print(\"Cover: \" + str(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of utility values\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "sns.scatterplot(data=vehicle_concept_designs_est_util, x=\"est_mean_util\", y=\"true_mean_util\")\n",
    "\n",
    "plt.xlabel(\"Estimated Utility\")\n",
    "\n",
    "plt.ylabel(\"True Utility\")\n",
    "\n",
    "#ax.set(xlabel='Estimated Utility', ylabel='True Utility')\n",
    "\n",
    "fig.savefig('Correlation_True_Est_Utility.png',bbox_inches='tight')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MSEs\n",
    "mu_target_mse = np.mean(np.square(target_dm_partworth - mu_target))\n",
    "\n",
    "prior_mean_mse = np.mean(np.square(target_dm_partworth - v_model_partworths))\n",
    "\n",
    "print(\"mu_target_mse: \" + str(mu_target_mse))\n",
    "print(\"prior_mean_mse: \" + str(prior_mean_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e25ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate correlations\n",
    "np.corrcoef(vehicle_concept_designs_est_util[\"est_mean_util\"],vehicle_concept_designs_est_util[\"true_mean_util\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
