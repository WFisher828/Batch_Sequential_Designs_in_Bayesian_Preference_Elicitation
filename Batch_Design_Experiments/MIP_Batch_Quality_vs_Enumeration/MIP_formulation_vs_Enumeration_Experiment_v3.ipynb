{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef4cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import math\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute expectation and variance of Z random variable parameterized by m and v\n",
    "\n",
    "def z_expectation_variance(m,v):\n",
    "    #m is question mean\n",
    "    #v is question variance\n",
    "    integ_bound = 30.0\n",
    "    \n",
    "    #Set up functions for calculating expectation and variance of Z\n",
    "    fun1 = lambda z: ((1 + math.e**(-m - math.sqrt(v)*z))**(-1))*(math.e**((-z**2)/2))/math.sqrt(2*math.pi)\n",
    "    fun2 = lambda z: z*((1 + math.e**(-m - math.sqrt(v)*z))**(-1))*(math.e**((-z**2)/2))/math.sqrt(2*math.pi)\n",
    "    fun3 = lambda z: z*z*((1 + math.e**(-m - math.sqrt(v)*z))**(-1))*(math.e**((-z**2)/2))/math.sqrt(2*math.pi)\n",
    "    \n",
    "    #Calculate expectation and variance of Z. C is the normalization constant that ensures the pdf of Z\n",
    "    #integrates to be 1. \n",
    "    C = scipy.integrate.quad(fun1, -integ_bound, integ_bound)[0]\n",
    "    mu_z = scipy.integrate.quad(fun2, -integ_bound, integ_bound)[0]/C\n",
    "    var_z = (scipy.integrate.quad(fun3, -integ_bound, integ_bound)[0] / C) - mu_z**2\n",
    "    \n",
    "    return [mu_z, var_z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8a9314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to generate an nxn matrix M(r) such that M_ij = r^|i-j| for -1<r<1. This matrix is called a \n",
    "#Kac-Murdock-Szego matrix.\n",
    "def KMS_Matrix(n,r):\n",
    "    #n: this is the number of rows and columns of the matrix\n",
    "    #r: this is the coefficient given above that determines the value of the matrice's entries \n",
    "    \n",
    "    \n",
    "    M = np.zeros([n,n])\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            M[i,j] = r**abs(i-j)\n",
    "        \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60770ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a set that has all the differences between binary products\n",
    "\n",
    "def product_diff_list(n):\n",
    "    #n: the number of attributes of the products\n",
    "    \n",
    "    #Example of itertools.product:\n",
    "    #itertools.product(range(2), repeat = 3) --> 000 001 010 011 100 101 110 111\n",
    "    p_d_l = list(itertools.product([-1.0,0.0,1.0],repeat = n))\n",
    "    \n",
    "    #Return the index of the tuple with all 0s.\n",
    "    zero_index = p_d_l.index(tuple([0]*n))\n",
    "\n",
    "    #Note that at this point, product_diff_list contains some redundant information. Due to\n",
    "    #the symmetry of the one-step and two-step acquisition function in terms of question mean, question pairs such as \n",
    "    #(-1,-1,-1,...,-1) and (1,1,1,...,1) (i.e. negative multiples) will evaluate as the same under the one-step and two-step\n",
    "    #acquisition functions. Due to the structure of product_diff_list, we can remove every question pair before and including\n",
    "    #the question pair with all zero entries in order to remove this redundant information.\n",
    "    for i in range(0,zero_index + 1):\n",
    "        p_d_l.pop(0)\n",
    "    \n",
    "    p_d_l = [np.array(a) for a in p_d_l]\n",
    "    return p_d_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given a trinary vector of 0, 1, and -1, find two binary products whose difference is the trinary vector.\n",
    "def question_extractor(prod):\n",
    "    #prod: This is a trinary vector of 0, 1, and -1 that represents the difference between two products, which\n",
    "    #are represented by binary vectors.\n",
    "    \n",
    "    x = [0]*len(prod)\n",
    "    y = [0]*len(prod)\n",
    "    for i in range(0,len(prod)):\n",
    "        if prod[i] == 1.0:\n",
    "            x[i] = 1.0\n",
    "            y[i] = 0.0\n",
    "        if prod[i] == 0.0:\n",
    "            x[i] = 0.0\n",
    "            y[i] = 0.0\n",
    "        if prod[i] == -1.0:\n",
    "            x[i] = 0.0\n",
    "            y[i] = 1.0\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea27a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_matching_update(x,y,mu_prior,Sig_prior):\n",
    "    #x and y are a question pair, x is preferred over y.\n",
    "    #mu_prior and Sig_prior are expectation and covariance matrix\n",
    "    #Make sure x, y, mu_prior, and Sig_prior are numpy arrays\n",
    "    x_vec = np.array(x)\n",
    "    y_vec = np.array(y)\n",
    "    mu_prior_vec = np.array(mu_prior)\n",
    "    Sig_prior_vec = np.array(Sig_prior)\n",
    "    \n",
    "    #Define question expectation and question variance\n",
    "    v = x_vec - y_vec\n",
    "    mu_v = np.dot(mu_prior_vec,v)\n",
    "    Sig_dot_v = np.dot(Sig_prior_vec,v)\n",
    "    Sig_v = np.dot(v,Sig_dot_v)\n",
    "    \n",
    "    #Save np.dot(Sig_prior_vec,v) as a variable (DONE)\n",
    "    \n",
    "    #Calculate expectation and variance for Z random variable\n",
    "    \n",
    "    [mu_z, var_z] = z_expectation_variance(mu_v,Sig_v)\n",
    "    \n",
    "    \n",
    "    #Calculate the update expectation and covariance matrix for \n",
    "    #posterior\n",
    "    mu_posterior = mu_prior_vec + (mu_z/math.sqrt(Sig_v))*Sig_dot_v\n",
    "    Sig_posterior = ((var_z-1)/Sig_v)*np.outer(Sig_dot_v,Sig_dot_v) + Sig_prior_vec\n",
    "    \n",
    "    return mu_posterior, Sig_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd7e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function constructs a batch design based off of average question mean, average question variance, and average\n",
    "#question orthogonality. For the average question orthogonality, we take the absolute value of the summands rather than\n",
    "#the square. We also normalize mu and Sig in the objective so that we do not need to keep on refitting the parameters \n",
    "#that go with question mean, question variance, and question orthogonality.\n",
    "\n",
    "def batch_design_AO(mu,Sig,batch_size,quest_mean_log_coeff,quest_var_log_coeff,quest_orth_log_coeff,t_lim = 100):\n",
    "    #mu: expectation of prior on beta\n",
    "    #Sig: Covariance matrix of prior on beta\n",
    "    #batch_size: the number of questions we want to return in our batch design. This should be less or equal to the number\n",
    "    #of attributes\n",
    "    #quest_mean_log_coeff: this is a fitting parameter that goes with the average question mean and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AM/||l*mu||\n",
    "    #quest_var_log_coeff: this is a fitting parameter that goes with the average question variance and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AV/||s*Sig||\n",
    "    #quest_orth_log_coeff: this is a fitting parameter that goes with the average question orthogonality and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AO/||s*Sig||\n",
    "    #(l,s) are scaling parameters for mu and Sig that divide the space into different signal-to-noise ratio regions.\n",
    "    #t_lim: this is the max amount of time we want to take to construct the batch\n",
    "    \n",
    "    #This is the number of attributes for the products\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('Timelimit',t_lim)\n",
    "    #Only save logfile for this experiment\n",
    "    m.setParam('LogFile',\"Batch_AO_batchsize\"+str(batch_size)+\"_meancoeff_\"+str(quest_mean_log_coeff)+\"_varcoeff_\"+\n",
    "               str(quest_var_log_coeff)+\"_orthcoeff_\"+str(quest_orth_log_coeff))\n",
    "    \n",
    "    #calculate 2-norms of mu and Sigma\n",
    "    mu_2norm = np.linalg.norm(mu,2)\n",
    "    Sig_2norm = np.linalg.norm(Sig,2)\n",
    "    \n",
    "    #List of tuples for delta variable\n",
    "    delta_tuples = []\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1,batch_size):\n",
    "            delta_tuples.append((i,j))\n",
    "    \n",
    "    #Set up the x_i and y_i, i = 1,...,batchsize\n",
    "    X = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    Y = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    Delta = m.addVars(delta_tuples, lb=0.0, vtype = GRB.CONTINUOUS)\n",
    "    \n",
    "    #Set up the objective function.\n",
    "    m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) + \n",
    "                   (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                   Y[i]@Sig@Y[i] for i in range(batch_size)]) + \n",
    "                   sum([(quest_orth_log_coeff/(batch_size*(batch_size-1)*Sig_2norm/2))*Delta[i,j] for i in range(batch_size) for j in range(i+1,batch_size)]),GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up the constraints that force the products in question i to be different, as well as forcing the symmetry\n",
    "    #exploitation condition.\n",
    "    for i in range(batch_size):\n",
    "        m.addConstr(X[i]@X[i] - X[i]@Y[i] - Y[i]@X[i] + Y[i]@Y[i] >= 1)\n",
    "        m.addConstr(mu@X[i] - mu@Y[i] >= 0)\n",
    "        \n",
    "    #Set up the Sigma-orthogonality constraint for all questions i and j, i not equal to j. Also add constraints\n",
    "    #to make sure that questions within a batch are different, including with respect to switching order of products in\n",
    "    #the questions.\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1,batch_size):\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] - Delta[i,j] <= 0)\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] + Delta[i,j] >= 0)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@X[j] + X[i]@Y[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@X[j] - Y[i]@Y[j] -\n",
    "                       X[j]@X[i] + X[j]@Y[i] + X[j]@X[j] - X[j]@Y[j] +\n",
    "                       Y[j]@X[i] - Y[j]@Y[i] - Y[j]@X[j] + Y[j]@Y[j] >= 1)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@Y[j] + X[i]@X[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@Y[j] - Y[i]@X[j] -\n",
    "                       Y[j]@X[i] + Y[j]@Y[i] + Y[j]@Y[j] - Y[j]@X[j] +\n",
    "                       X[j]@X[i] - X[j]@Y[i] - X[j]@Y[j] + X[j]@X[j] >= 1)\n",
    "            \n",
    "    m.optimize()\n",
    "    \n",
    "    #This will be the list of products\n",
    "    Q = [ [] for i in range(batch_size)]\n",
    "    D = [ [] for i in range(batch_size-1)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        Q[i].append(X[i].X)\n",
    "        Q[i].append(Y[i].X)\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1, batch_size):\n",
    "            D[i].append(Delta[i,j].X)\n",
    "        \n",
    "    return[Q,D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f465e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function constructs a batch design based off of average question mean, average question variance, and MAXIMUM\n",
    "#question orthogonality. For the average question orthogonality, we take the absolute value of the summands rather than\n",
    "#the square. We also normalize mu and Sig in the objective so that we do not need to keep on refitting the parameters \n",
    "#that go with question mean, question variance, and question orthogonality.\n",
    "\n",
    "def batch_design_MO(mu,Sig,batch_size,quest_mean_log_coeff,quest_var_log_coeff,quest_orth_log_coeff,t_lim = 100):\n",
    "    #mu: expectation of prior on beta\n",
    "    #Sig: Covariance matrix of prior on beta\n",
    "    #batch_size: the number of questions we want to return in our batch design. This should be less or equal to the number\n",
    "    #of attributes\n",
    "    #quest_mean_log_coeff: this is a fitting parameter that goes with the average question mean and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + MO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AM/||l*mu||\n",
    "    #quest_var_log_coeff: this is a fitting parameter that goes with the average question variance and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + MO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AV/||s*Sig||\n",
    "    #quest_orth_log_coeff: this is a fitting parameter that goes with the average question orthogonality and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + MO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #MO/||s*Sig||\n",
    "    #(l,s) are scaling parameters for mu and Sig that divide the space into different signal-to-noise ratio regions.\n",
    "    #t_lim: this is the max amount of time we want to take to construct the batch\n",
    "    \n",
    "    #This is the number of attributes for the products\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('Timelimit',t_lim)\n",
    "    #Only save logfile for this experiment\n",
    "    m.setParam('LogFile',\"Batch_MO_batchsize\"+str(batch_size)+\"_meancoeff_\"+str(quest_mean_log_coeff)+\"_varcoeff_\"+\n",
    "               str(quest_var_log_coeff)+\"_orthcoeff_\"+str(quest_orth_log_coeff))\n",
    "    \n",
    "    #calculate 2-norms of mu and Sigma\n",
    "    mu_2norm = np.linalg.norm(mu,2)\n",
    "    Sig_2norm = np.linalg.norm(Sig,2)\n",
    "    \n",
    "    #Set up the x_i and y_i, i = 1,...,batchsize\n",
    "    X = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    Y = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    delta = m.addVar(lb=0.0, vtype = GRB.CONTINUOUS)\n",
    "    \n",
    "    #set up the objective function\n",
    "    m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) +\n",
    "                  (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                   Y[i]@Sig@Y[i] for i in range(batch_size)]) + (quest_orth_log_coeff/Sig_2norm)*delta,GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up the constraints that force the products in question i to be different, as well as forcing the symmetry\n",
    "    #exploitation condition.\n",
    "    for i in range(batch_size):\n",
    "        m.addConstr(X[i]@X[i] - X[i]@Y[i] - Y[i]@X[i] + Y[i]@Y[i] >= 1)\n",
    "        m.addConstr(mu@X[i] - mu@Y[i] >= 0)\n",
    "    \n",
    "    #Set up the Sigma-orthogonality constraint for all questions i and j, i not equal to j. Also add constraints\n",
    "    #to make sure that questions within a batch are different, including with respect to switching order of products in\n",
    "    #the questions.\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1,batch_size):\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] - delta <= 0)\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] + delta >= 0)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@X[j] + X[i]@Y[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@X[j] - Y[i]@Y[j] -\n",
    "                       X[j]@X[i] + X[j]@Y[i] + X[j]@X[j] - X[j]@Y[j] +\n",
    "                       Y[j]@X[i] - Y[j]@Y[i] - Y[j]@X[j] + Y[j]@Y[j] >= 1)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@Y[j] + X[i]@X[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@Y[j] - Y[i]@X[j] -\n",
    "                       Y[j]@X[i] + Y[j]@Y[i] + Y[j]@Y[j] - Y[j]@X[j] +\n",
    "                       X[j]@X[i] - X[j]@Y[i] - X[j]@Y[j] + X[j]@X[j] >= 1)\n",
    "    \n",
    "    m.optimize()\n",
    "    \n",
    "    #This will be the list of products\n",
    "    Q = [ [] for i in range(batch_size)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        Q[i].append(X[i].X)\n",
    "        Q[i].append(Y[i].X)\n",
    "        \n",
    "    return[Q,delta.X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to generate data to estimate the parameters in the normalized AO model. The normalized AO model\n",
    "#is given by log(D-err/Det^(1/2)(Sig)) ~ AM/||L*mu|| + AV/||S*Sig|| + AO/||S*Sig|| + ||L*mu|| + ||S*Sig||. AM, AV, and AO denote\n",
    "#the average question mean, average quesiton variance, and average question orthogonality of a given design under prior\n",
    "#N(mu, Sig). L and S denote varying signal and noise levels, respectively. The normalized AO model is used in our\n",
    "#optimization procedure so that we will not have to refit the parameters in the optimization model everytime the user\n",
    "#answers a batch. The idea is that varying L and S enough should encompass a wide enough range so that mu and Sig will\n",
    "#be within this range after updating.\n",
    "#In this function, we also include MO so that we may fit a maximum orthogonality model.\n",
    "\n",
    "#!!! rng will need to be set before calling this function !!!\n",
    "\n",
    "def norm_AO_MO_data_generation(init_mu, init_Sig, batch_size, L, S, num_random_batches, num_true_partworths):\n",
    "    #init_mu: This is the initial expectation of the partworths.\n",
    "    #init_Sig: This is the initial covariance matrix of the partworths.\n",
    "    #batch_size: This is the number of questions in each batch\n",
    "    #L: This is a vector which holds varying levels of signal (multiply with mu). For example,\n",
    "    #we could have L = [0.25,1.0,4.0]\n",
    "    #S: This is a vector which holds varying levels of noise (multiply with Sig). For example,\n",
    "    #we could have S = [0.25,1.0,4.0]\n",
    "    #num_random_batches: This is the number of random batches that we will generate for collecting data on log(D-err),\n",
    "    #AM, AV, and AO (and MO). This set of random batches will be used for each level combination of L and S.\n",
    "    #num_true_partworths: This is the number of true/baseline partworths we will use to evaluate the d-error of a design.\n",
    "    \n",
    "    attr_num = len(init_mu)\n",
    "    \n",
    "    #Create lists to store average orthogonality and max orthogonality, as well as d-error and average question mean and\n",
    "    #average question variance, and ||L*mu|| and ||S*Sig|| as well.\n",
    "    average_orthogonality = []\n",
    "    \n",
    "    maximum_orthogonality = []\n",
    "    \n",
    "    average_question_mean = []\n",
    "    \n",
    "    average_question_variance = []\n",
    "    \n",
    "    average_d_error = []\n",
    "    \n",
    "    L_mu = []\n",
    "    \n",
    "    S_Sig = []\n",
    "    \n",
    "    init_sqrt_determinant = []\n",
    "    \n",
    "    #Create a list of all products.\n",
    "    prod_list = product_diff_list(attr_num)\n",
    "    \n",
    "    #Construct the set of batch designs\n",
    "    batch_set = [[] for i in range(num_random_batches)]\n",
    "    for i in range(num_random_batches):\n",
    "        random_question_matrix = random.sample(prod_list,batch_size)\n",
    "        for m in range(batch_size):\n",
    "            #random_question_vec = random.sample(prod_list,1)[0]\n",
    "            #[x,y] = question_extractor(random_question_vec)\n",
    "            [x,y] = question_extractor(random_question_matrix[m])\n",
    "            batch_set[i].append([x,y])\n",
    "    \n",
    "    #Record the scaled norm of mu and Sig for each combination of L and S\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                L_mu.append(l*np.linalg.norm(init_mu,2))\n",
    "                S_Sig.append(s*np.linalg.norm(init_Sig,2))\n",
    "                init_sqrt_determinant.append(np.sqrt(np.linalg.det(s*init_Sig)))\n",
    "    \n",
    "    #Calculate AM, AV, AO and MO for each of the batches\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                random_batch_question_mean = []\n",
    "                random_batch_question_variance = []\n",
    "                random_batch_orthogonality = []\n",
    "        \n",
    "                for p in range(batch_size):\n",
    "                    x_p = np.array(batch_set[i][p][0])\n",
    "                    y_p = np.array(batch_set[i][p][1])\n",
    "                    random_batch_question_mean.append(np.abs(np.dot(l*init_mu,x_p - y_p)))\n",
    "                    random_batch_question_variance.append(np.dot(x_p - y_p, np.dot(s*init_Sig,x_p - y_p)))\n",
    "                    for q in range(p+1, batch_size):\n",
    "                        x_q = np.array(batch_set[i][q][0])\n",
    "                        y_q = np.array(batch_set[i][q][1])\n",
    "                        random_batch_orthogonality.append(np.abs(np.dot(x_p - y_p, np.dot(s*init_Sig,x_q - y_q))))\n",
    "                \n",
    "                #We use this if statement in case the batch size is 1.\n",
    "                if len(random_batch_orthogonality) > 0:\n",
    "                    average_orthogonality.append(np.mean(np.array(random_batch_orthogonality)))\n",
    "                    maximum_orthogonality.append(np.max(np.array(random_batch_orthogonality)))\n",
    "        \n",
    "                average_question_mean.append(np.mean(np.array(random_batch_question_mean)))\n",
    "                average_question_variance.append(np.mean(np.array(random_batch_question_variance)))\n",
    "            \n",
    "    #Calculate the D-error.\n",
    "    for l in L:\n",
    "        #print('L: '+str(l))\n",
    "        for s in S:\n",
    "            #print('S: '+str(s))\n",
    "            true_partworths = []\n",
    "            for t in range(num_true_partworths):\n",
    "                true_partworths.append(rng.multivariate_normal(l*init_mu,s*init_Sig))\n",
    "            \n",
    "            gumbel_errors = [[[np.random.gumbel(0,1) for k in range(2)] for j in range(batch_size)] for i in range(num_true_partworths)]\n",
    "            \n",
    "            for i in range(num_random_batches):\n",
    "                #Create a list for the batch that will store the final determinant value for each simulation\n",
    "                #corresponding to each baseline partworth.\n",
    "                batch_simulate_d_values = []\n",
    "                \n",
    "                #Simulate d-efficiency over baseline partworths\n",
    "                for j in range(len(true_partworths)):\n",
    "                #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "                    mu = l*init_mu\n",
    "                    Sig = s*init_Sig\n",
    "                    \n",
    "                    #Each simulation goes through the questions in the random batch.\n",
    "                    for k in range(batch_size):\n",
    "                    #Set x and y\n",
    "                        x = batch_set[i][k][0]\n",
    "                        y = batch_set[i][k][1]\n",
    "                \n",
    "                        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "                        x_temp = x\n",
    "                        y_temp = y\n",
    "                        \n",
    "                        gum_x = gumbel_errors[j][k][0]\n",
    "                        gum_y = gumbel_errors[j][k][1]\n",
    "                        #See preference between two products\n",
    "                        if (np.dot(true_partworths[j],np.array(y)) + gum_y) >= (np.dot(true_partworths[j],np.array(x)) + gum_x):\n",
    "                            x = y_temp\n",
    "                            y = x_temp\n",
    "                            \n",
    "                        #Perform moment matching after choice is made.\n",
    "                        [mu, Sig] = moment_matching_update(x,y,mu,Sig)\n",
    "                        \n",
    "                    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "                    #of the final covariance matrix.\n",
    "                    batch_simulate_d_values.append(np.sqrt(np.linalg.det(Sig)))\n",
    "                    \n",
    "                #We average the d-values from the simulation for a batch and store it in a list.\n",
    "                average_d_error.append(np.mean(batch_simulate_d_values))\n",
    "                \n",
    "    return average_orthogonality, maximum_orthogonality, average_question_mean, average_question_variance, L_mu, S_Sig, init_sqrt_determinant, average_d_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105ad21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to create a set of batch designs and evaluate their D-error (in a sequential manner). \n",
    "#We save the D-error of each batch in a list.\n",
    "\n",
    "def random_batch_D_error(init_mu,init_Sig,batch_size,num_random_batches,true_partworths, gumbel_error_terms):\n",
    "    #init_mu: This is the initial expectation of the partworths.\n",
    "    #init_Sig: This is the initial covariance matrix of the partworths.\n",
    "    #batch_size: This is the number of questions in each batch.\n",
    "    #num_random_batches: This is the number of random batches that we will generate.\n",
    "    #true_partworths: This is the true/baseline partworths we will use to evaluate the d-error of a design.\n",
    "    #gumbel_error_terms: This is a list of gumbel errors terms used in evaluating the d-error of a design. This \n",
    "    #list should have dimension (true_partworths x batch_size), where each entry is a list with two randomly generated\n",
    "    #gumbel terms. We use the same true partworths and error terms for evaluating the d-error of each randomly generated \n",
    "    #design.\n",
    "    \n",
    "    attr_num = len(init_mu)\n",
    "    \n",
    "    average_d_error = []\n",
    "    \n",
    "    #Create a list of all products.\n",
    "    prod_list = product_diff_list(attr_num)\n",
    "    \n",
    "    #Construct the set of batch designs\n",
    "    batch_set = [[] for i in range(num_random_batches)]\n",
    "    for i in range(num_random_batches):\n",
    "        random_question_matrix = random.sample(prod_list,batch_size)\n",
    "        for m in range(batch_size):\n",
    "            #random_question_vec = random.sample(prod_list,1)[0]\n",
    "            #[x,y] = question_extractor(random_question_vec)\n",
    "            [x,y] = question_extractor(random_question_matrix[m])\n",
    "            batch_set[i].append([x,y])\n",
    "        \n",
    "            \n",
    "    #Evaluate the d-error\n",
    "    #true_partworths = []\n",
    "    #for t in range(num_true_partworths):\n",
    "                #true_partworths.append(rng.multivariate_normal(init_mu,init_Sig))\n",
    "    num_true_partworths = len(true_partworths)\n",
    "    \n",
    "    for i in range(num_random_batches):\n",
    "                #Create a list for the batch that will store the final determinant value for each simulation\n",
    "                #corresponding to each baseline partworth.\n",
    "                batch_simulate_d_values = []\n",
    "                #print('random_batch_number: ' + str(i))\n",
    "                #Simulate d-efficiency over baseline partworths\n",
    "                for j in range(num_true_partworths):\n",
    "                #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "                    mu = init_mu\n",
    "                    Sig = init_Sig\n",
    "                    \n",
    "                    #Each simulation goes through the questions in the random batch.\n",
    "                    for k in range(batch_size):\n",
    "                    #Set x and y\n",
    "                        x = batch_set[i][k][0]\n",
    "                        y = batch_set[i][k][1]\n",
    "                \n",
    "                        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "                        x_temp = x\n",
    "                        y_temp = y\n",
    "                        \n",
    "                        #See preference between two products.\n",
    "                        gum_x = gumbel_error_terms[j][k][0]#np.random.gumbel(0,1)\n",
    "                        gum_y = gumbel_error_terms[j][k][1]#np.random.gumbel(0,1)\n",
    "                        if (np.dot(true_partworths[j],np.array(y)) + gum_y) >= (np.dot(true_partworths[j],np.array(x)) + gum_x):\n",
    "                            x = y_temp\n",
    "                            y = x_temp\n",
    "                            \n",
    "                        #Perform moment matching after choice is made.\n",
    "                        [mu, Sig] = moment_matching_update(x,y,mu,Sig)\n",
    "                        \n",
    "                    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "                    #of the final covariance matrix.\n",
    "                    batch_simulate_d_values.append(np.sqrt(np.linalg.det(Sig)))\n",
    "                    \n",
    "                #We average the d-values from the simulation for a batch and store it in a list.\n",
    "                average_d_error.append(np.mean(batch_simulate_d_values))\n",
    "                \n",
    "    return average_d_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187bdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the models batch_AO and batch_MO.\n",
    "rng = np.random.default_rng(100) \n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "\n",
    "#signal to noise ratio. \n",
    "#1 - LOW: multiply expectation by 0.25 and covariance by 4.0\n",
    "#2 - REG: multiply expectation by 1.0 and covariance by 1.0\n",
    "#3 - HIGH: multiply expectation by 4.0 and covariance by 0.25\n",
    "snr = int(sys.argv[1])\n",
    "\n",
    "\n",
    "#Prior type\n",
    "#1 - homogeneous expectation and identity covariance matrix\n",
    "#2 - heterogeneous expectation and KMS covariance matrix\n",
    "prior_type = int(sys.argv[2])\n",
    "\n",
    "if snr == 1:\n",
    "    if prior_type == 1:\n",
    "        init_mu_fit = 0.25*np.array(6*[1.0])\n",
    "        init_Sig_fit = 4.0*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        init_mu_fit = 0.25*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        init_Sig_fit = 4.0*(1.25*KMS_Matrix(6,-0.5))\n",
    "if snr == 2:\n",
    "    if prior_type == 1:\n",
    "        init_mu_fit = 1.0*np.array(6*[1.0])\n",
    "        init_Sig_fit = 1.0*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        init_mu_fit = 1.0*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        init_Sig_fit = 1.0*(1.25*KMS_Matrix(6,-0.5))\n",
    "if snr == 3:\n",
    "    if prior_type == 1:\n",
    "        init_mu_fit = 4.0*np.array(6*[1.0])\n",
    "        init_Sig_fit = 0.25*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        init_mu_fit = 4.0*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        init_Sig_fit = 0.25*(1.25*KMS_Matrix(6,-0.5))\n",
    "    \n",
    "batch_size_fit = 4\n",
    "    \n",
    "L_fit = [0.5,1.0,2.0]\n",
    "S_fit = [0.5,1.0,2.0]\n",
    "\n",
    "num_random_batches_fit = 1000\n",
    "num_true_partworths_fit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the data in order to estimate the parameters of the AO and MO models\n",
    "average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit = norm_AO_MO_data_generation(init_mu_fit, init_Sig_fit, batch_size_fit, L_fit, S_fit, num_random_batches_fit, num_true_partworths_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe of the generated data for fitting the parameters of the AO and MO models\n",
    "df_fit = pd.DataFrame(list(zip(average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit)),\n",
    "                  columns =['Avg_Orth', 'Max_Orth', 'Avg_Quest_Mean', 'Avg_Quest_Var', 'L_mu_norm', 'S_Sig_norm', 'Init_Sqrt_Det', 'D_err'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca86ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add some new columns to the dataset. We mean-center the independent variables to attempt to reduce VIF. This will not affect the value of\n",
    "#of the coefficients, except for the intercept.\n",
    "df_fit['log_norm_derr'] = np.log(np.divide(np.array(df_fit['D_err']),np.array(df_fit['Init_Sqrt_Det'])))\n",
    "df_fit['cent_norm_AM'] = np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])))\n",
    "df_fit['cent_norm_AV'] = np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_AO'] = np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_MO'] = np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "\n",
    "df_fit['cent_L_mu_norm'] = df_fit['L_mu_norm'] - np.mean(np.array(df_fit['L_mu_norm']))\n",
    "df_fit['cent_S_Sig_norm'] = df_fit['S_Sig_norm'] - np.mean(np.array(df_fit['S_Sig_norm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806792f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save resulting file as a CSV.\n",
    "if snr == 1:\n",
    "    if prior_type == 1:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_mu025_Sig4Ident_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_lowsnr_muhet_SigKMS_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')\n",
    "if snr == 2:\n",
    "    if prior_type == 1:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_mu1_Sig1Ident_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_medsnr_muhet_SigKMS_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')\n",
    "if snr == 3:\n",
    "    if prior_type == 1:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_mu4_Sig025Ident_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        df_fit.to_csv('MIPvEnum_Normalized_AO_MO_Model_Data_highsnr_muhet_SigKMS_batchsize4_L_05_1_2_S_05_1_2_nrb_1000_ntp_50_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7700d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with AO\n",
    "model_AO = sm.formula.ols(formula = \"log_norm_derr ~  cent_norm_AM + cent_norm_AV + cent_norm_AO + cent_L_mu_norm + cent_S_Sig_norm\", data = df_fit).fit()\n",
    "parameter_est_AO = model_AO.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model with MO\n",
    "model_MO = sm.formula.ols(formula = \"log_norm_derr ~  cent_norm_AM + cent_norm_AV + cent_norm_MO + cent_L_mu_norm + cent_S_Sig_norm\", data = df_fit).fit()\n",
    "parameter_est_MO = model_MO.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc55ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model parameters in txt file\n",
    "if snr == 1:\n",
    "    params_est = np.array([parameter_est_AO[1],parameter_est_AO[2],parameter_est_AO[3],parameter_est_MO[1],parameter_est_MO[2],parameter_est_MO[3]])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_modelparams_lowsnr_v3.txt',params_est)\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_modelparams_lowsnr_muhet_SigKMS_v3.txt', params_est)\n",
    "if snr == 2:\n",
    "    params_est = np.array([parameter_est_AO[1],parameter_est_AO[2],parameter_est_AO[3],parameter_est_MO[1],parameter_est_MO[2],parameter_est_MO[3]])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_modelparams_medsnr_v3.txt',params_est)\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_modelparams_medsnr_muhet_SigKMS_v3.txt', params_est)\n",
    "if snr == 3:\n",
    "    params_est = np.array([parameter_est_AO[1],parameter_est_AO[2],parameter_est_AO[3],parameter_est_MO[1],parameter_est_MO[2],parameter_est_MO[3]])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_modelparams_highsnr_v3.txt',params_est)\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_modelparams_highsnr_muhet_SigKMS_v3.txt', params_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44cd96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment settings:\n",
    "#Settings for experiment\n",
    "rng = np.random.default_rng(100)\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "\n",
    "    \n",
    "if snr == 1:\n",
    "    if prior_type == 1:\n",
    "        mu_exp = 0.25*np.array(6*[1.0])\n",
    "        Sig_exp = 4.0*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        mu_exp = 0.25*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        Sig_exp = 4.0*(1.25*KMS_Matrix(6,-0.5))\n",
    "if snr == 2:\n",
    "    if prior_type == 1:\n",
    "        mu_exp = 1.0*np.array(6*[1.0])\n",
    "        Sig_exp = 1.0*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        mu_exp = 1.0*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        Sig_exp = 1.0*(1.25*KMS_Matrix(6,-0.5))\n",
    "if snr == 3:\n",
    "    if prior_type == 1:\n",
    "        mu_exp = 4.0*np.array(6*[1.0])\n",
    "        Sig_exp = 0.25*np.identity(6)\n",
    "    if prior_type == 2:\n",
    "        mu_exp = 4.0*np.array([-0.25,0.5,-0.75,1.0,-1.25,1.5])\n",
    "        Sig_exp = 0.25*(1.25*KMS_Matrix(6,-0.5))\n",
    "\n",
    "batch_size_exp = 4\n",
    "num_random_batches_exp = 10000        \n",
    "\n",
    "true_partworth_exp = []\n",
    "\n",
    "num_true_partworths_exp = 100\n",
    "#There will be 100 true partworths\n",
    "for t in range(num_true_partworths_exp):\n",
    "    true_partworth_exp.append(rng.multivariate_normal(mu_exp,Sig_exp))\n",
    "    \n",
    "#Create list of gumbel error terms\n",
    "gumbel_error_terms_exp = [[[np.random.gumbel(0,1) for k in range(2)] for j in range(batch_size_exp)] for i in range(num_true_partworths_exp)]\n",
    "print(gumbel_error_terms_exp[0])   \n",
    "\n",
    "AO_alpha_exp = parameter_est_AO[1]\n",
    "AO_kappa_exp = parameter_est_AO[2]\n",
    "AO_gamma_exp = parameter_est_AO[3]\n",
    "\n",
    "MO_alpha_exp = parameter_est_MO[1]\n",
    "MO_kappa_exp = parameter_est_MO[2]\n",
    "MO_gamma_exp = parameter_est_MO[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0209db24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create list of d-error of random batch designs\n",
    "tic = time.perf_counter()\n",
    "d_err_list = random_batch_D_error(mu_exp,Sig_exp,batch_size_exp,num_random_batches_exp,true_partworth_exp,gumbel_error_terms_exp)\n",
    "toc = time.perf_counter()\n",
    "time_meas = toc - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b08463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Batch_AO design and evaluate its D-error\n",
    "[AO_batch,AO_Ortho] = batch_design_AO(mu_exp,Sig_exp,batch_size_exp,AO_alpha_exp,AO_kappa_exp,AO_gamma_exp,t_lim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acea520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Batch_MO design and evaluate its D-error\n",
    "[MO_batch, MO_Ortho] = batch_design_MO(mu_exp,Sig_exp,batch_size_exp,MO_alpha_exp,MO_kappa_exp,MO_gamma_exp,t_lim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c231d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the d-error of the Batch_AO design\n",
    "\n",
    "#Create a list for the batch that will store the final determinant value for each simulation\n",
    "#corresponding to each baseline partworth.\\\n",
    "np.random.seed(100) #Add this so that gumbel error terms \n",
    "batch_AO_d_values = []\n",
    "                \n",
    "#Simulate d-efficiency over baseline partworths\n",
    "for j in range(len(true_partworth_exp)):\n",
    "    #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "    mu_AO = mu_exp\n",
    "    Sig_AO = Sig_exp\n",
    "                    \n",
    "    #Each simulation goes through the questions in the random batch.\n",
    "    for k in range(batch_size_exp):\n",
    "        #Set x and y\n",
    "        x_AO = AO_batch[k][0]\n",
    "        y_AO = AO_batch[k][1]\n",
    "                \n",
    "        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "        x_temp_AO = x_AO\n",
    "        y_temp_AO = y_AO\n",
    "        \n",
    "        gum_x_AO = gumbel_error_terms_exp[j][k][0]\n",
    "        gum_y_AO = gumbel_error_terms_exp[j][k][1]\n",
    "        #See preference between two products.\n",
    "        if (np.dot(true_partworth_exp[j],np.array(y_AO)) + gum_y_AO) >= (np.dot(true_partworth_exp[j],np.array(x_AO)) + gum_x_AO):\n",
    "            x_AO = y_temp_AO\n",
    "            y_AO = x_temp_AO\n",
    "                            \n",
    "        #Perform moment matching after choice is made.\n",
    "        [mu_AO, Sig_AO] = moment_matching_update(x_AO,y_AO,mu_AO,Sig_AO)\n",
    "                        \n",
    "    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "    #of the final covariance matrix.\n",
    "    batch_AO_d_values.append(np.sqrt(np.linalg.det(Sig_AO)))\n",
    "                    \n",
    "#We average the d-values from the simulation for the AO batch and save it as a variable.\n",
    "batch_AO_d_error = np.mean(batch_AO_d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4206101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the d-error of the Batch_MO design\n",
    "\n",
    "#Create a list for the batch that will store the final determinant value for each simulation\n",
    "#corresponding to each baseline partworth.\n",
    "np.random.seed(100) #Add this so that gumbel error terms will be the same as those used in AO.\n",
    "batch_MO_d_values = []\n",
    "                \n",
    "#Simulate d-efficiency over baseline partworths\n",
    "for j in range(len(true_partworth_exp)):\n",
    "    #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "    mu_MO = mu_exp\n",
    "    Sig_MO = Sig_exp\n",
    "                    \n",
    "    #Each simulation goes through the questions in the random batch.\n",
    "    for k in range(batch_size_exp):\n",
    "        #Set x and y\n",
    "        x_MO = MO_batch[k][0]\n",
    "        y_MO = MO_batch[k][1]\n",
    "                \n",
    "        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "        x_temp_MO = x_MO\n",
    "        y_temp_MO = y_MO\n",
    "        \n",
    "        gum_x_MO = gumbel_error_terms_exp[j][k][0]\n",
    "        gum_y_MO = gumbel_error_terms_exp[j][k][1]\n",
    "        #See preference between two products. \n",
    "        if (np.dot(true_partworth_exp[j],np.array(y_MO)) + gum_y_MO) >= (np.dot(true_partworth_exp[j],np.array(x_MO)) + gum_x_MO):\n",
    "            x_MO = y_temp_MO\n",
    "            y_MO = x_temp_MO\n",
    "                            \n",
    "        #Perform moment matching after choice is made.\n",
    "        [mu_MO, Sig_MO] = moment_matching_update(x_MO,y_MO,mu_MO,Sig_MO)\n",
    "                        \n",
    "    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "    #of the final covariance matrix.\n",
    "    batch_MO_d_values.append(np.sqrt(np.linalg.det(Sig_MO)))\n",
    "                    \n",
    "#We average the d-values from the simulation for the AO batch and save it as a variable.\n",
    "batch_MO_d_error = np.mean(batch_MO_d_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_AO_success = [1 if batch_AO_d_error < rand_batch_d_error else 0 for rand_batch_d_error in d_err_list]\n",
    "batch_MO_success = [1 if batch_MO_d_error < rand_batch_d_error else 0 for rand_batch_d_error in d_err_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9449a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save d-error information and success information in a csv\n",
    "batch_information_list = np.array([d_err_list,batch_AO_success,batch_MO_success]).T\n",
    "batch_information_list_df = pd.DataFrame(batch_information_list, columns = ['D-errors', 'Batch AO < D-error', 'Batch MO < D-error'])\n",
    "\n",
    "if snr == 1:\n",
    "    if prior_type == 1:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_lowsnr_d_error_batch_comparison_list_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_lowsnr_d_error_batch_comparison_list_muhet_SigKMS_v3.csv')\n",
    "if snr == 2:\n",
    "    if prior_type == 1:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_mediumsnr_d_error_batch_comparison_list_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_mediumsnr_d_error_batch_comparison_list_muhet_SigKMS_v3.csv')\n",
    "if snr == 3:\n",
    "    if prior_type == 1:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_highsnr_d_error_batch_comparison_list_v3.csv')\n",
    "    if prior_type == 2:\n",
    "        batch_information_list_df.to_csv('MIPvsEnum_highsnr_d_error_batch_comparison_list_muhet_SigKMS_v3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc80f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct estimates and confidence interval\n",
    "AO_prop_est = np.mean(batch_AO_success)\n",
    "MO_prop_est = np.mean(batch_MO_success)\n",
    "\n",
    "AO_CI_upper = AO_prop_est + 1.96*np.sqrt(AO_prop_est*(1-AO_prop_est)/len(batch_AO_success))\n",
    "AO_CI_lower = AO_prop_est - 1.96*np.sqrt(AO_prop_est*(1-AO_prop_est)/len(batch_AO_success))\n",
    "\n",
    "MO_CI_upper = MO_prop_est + 1.96*np.sqrt(MO_prop_est*(1-MO_prop_est)/len(batch_MO_success))\n",
    "MO_CI_lower = MO_prop_est - 1.96*np.sqrt(MO_prop_est*(1-MO_prop_est)/len(batch_MO_success))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16183f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save confidence interval information, time for iterating over random batches, and d-error of batches\n",
    "if snr == 1:\n",
    "    CI_information = np.array(['batch_AO_d_error: ' + str(batch_AO_d_error),'AO_prop_est: ' +str(AO_prop_est),'AO_CI_lower: ' + str(AO_CI_lower),'AO_CI_upper: ' + str(AO_CI_upper),'batch_MO_d_error: ' + str(batch_MO_d_error),'MO_prop_est: ' + str(MO_prop_est),'MO_CI_lower:' + str(MO_CI_lower),'MO_CI_upper: ' + str(MO_CI_upper),'time for enumeration: ' + str(time_meas)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_CIinfo_lowsnr_v3.txt',CI_information,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_CIinfo_lowsnr_muhet_SigKMS_v3.txt',CI_information,fmt='%s')\n",
    "if snr == 2:\n",
    "    CI_information = np.array(['batch_AO_d_error: ' + str(batch_AO_d_error),'AO_prop_est: ' +str(AO_prop_est),'AO_CI_lower: ' + str(AO_CI_lower),'AO_CI_upper: ' + str(AO_CI_upper),'batch_MO_d_error: ' + str(batch_MO_d_error),'MO_prop_est: ' + str(MO_prop_est),'MO_CI_lower:' + str(MO_CI_lower),'MO_CI_upper: ' + str(MO_CI_upper),'time for enumeration: ' + str(time_meas)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_CIinfo_mediumsnr_v3.txt',CI_information,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_CIinfo_mediumsnr_muhet_SigKMS_v3.txt',CI_information,fmt='%s')\n",
    "if snr == 3:\n",
    "    CI_information = np.array(['batch_AO_d_error: ' + str(batch_AO_d_error),'AO_prop_est: ' +str(AO_prop_est),'AO_CI_lower: ' + str(AO_CI_lower),'AO_CI_upper: ' + str(AO_CI_upper),'batch_MO_d_error: ' + str(batch_MO_d_error),'MO_prop_est: ' + str(MO_prop_est),'MO_CI_lower:' + str(MO_CI_lower),'MO_CI_upper: ' + str(MO_CI_upper),'time for enumeration: ' + str(time_meas)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_CIinfo_highsnr_v3.txt',CI_information,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_CIinfo_highsnr_muhet_SigKMS_v3.txt',CI_information,fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save information regarding the batch design.\n",
    "if snr == 1:\n",
    "    batch_info = np.array(['AO_batch: '+str(AO_batch),'AO_Orth: '+str(AO_Ortho),'MO_batch: '+str(MO_batch),'MO_Orth: '+str(MO_Ortho)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_batchinfo_lowsnr_v3.txt',batch_info,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_batchinfo_lowsnr_muhet_SigKMS_v3.txt',batch_info,fmt='%s')\n",
    "if snr == 2:\n",
    "    batch_info = np.array(['AO_batch: '+str(AO_batch),'AO_Orth: '+str(AO_Ortho),'MO_batch: '+str(MO_batch),'MO_Orth: '+str(MO_Ortho)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_batchinfo_mediumsnr_v3.txt',batch_info,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_batchinfo_mediumsnr_muhet_SigKMS_v3.txt',batch_info,fmt='%s')\n",
    "if snr == 3:\n",
    "    batch_info = np.array(['AO_batch: '+str(AO_batch),'AO_Orth: '+str(AO_Ortho),'MO_batch: '+str(MO_batch),'MO_Orth: '+str(MO_Ortho)])\n",
    "    if prior_type == 1:\n",
    "        np.savetxt('MIPvEnum_batchinfo_highsnr_v3.txt',batch_info,fmt='%s')\n",
    "    if prior_type == 2:\n",
    "        np.savetxt('MIPvEnum_batchinfo_highsnr_muhet_SigKMS_v3.txt',batch_info,fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
