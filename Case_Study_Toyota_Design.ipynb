{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b0be10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "from Helper_Functions import z_expectation_variance,moment_matching_update,product_diff_list,question_extractor\n",
    "\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pylogit as pl\n",
    "from collections import OrderedDict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from choicedesign.design import EffDesign\n",
    "#from biogeme.expressions import Beta, Variable\n",
    "#import biogeme.database as db\n",
    "#import biogeme.models as models\n",
    "\n",
    "#import pyDOE2\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd915a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5962c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function simulates a decision maker picking between objects x and y.\n",
    "def simulated_decision_maker_selection(true_partworth,x,y):\n",
    "    #true_partworth: this is a numpy array of partworths of the decision maker\n",
    "    #x and y: two objects for the decision maker to compare, should be numpy arrays. \n",
    "    \n",
    "    preferred_option = x\n",
    "    not_preferred = y\n",
    "    epsilon_y = rng.gumbel(loc = 0.0, scale = 1.0)\n",
    "    epsilon_x = rng.gumbel(loc = 0.0, scale = 1.0)\n",
    "    \n",
    "    \n",
    "    if np.dot(true_partworth,y) + epsilon_y > np.dot(true_partworth,x) + epsilon_x:\n",
    "        preferred_option = y\n",
    "        not_preferred = x\n",
    "    \n",
    "    return preferred_option,not_preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eed040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import a choice design for the simulated decision maker to participate in. This will be used to construct a prior\n",
    "prior_design_data = pd.read_csv('Toyota_Corolla_Prior_Design.csv', header = 0)\n",
    "prior_design_data.head() # to display the first 5 lines of loaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b95f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we simulate a group of individuals participating in a static questionnaire and we will end up using the\n",
    "#estimates of the model parameters from this study as prior information for our decision maker of interest.\n",
    "\n",
    "#Specify group mean and variance\n",
    "group_pref_mean = np.array([1.0,-0.5,-0.25,0.0,0.0,0.5,-0.5,-0.5,0.0,0.0,0.0])\n",
    "#group_pref_var = np.array([[0.25, 0.125, -0.0625, 0,0,0,0,0,0,0,0],\n",
    "                           #[0.125, 0.25, -0.125, 0,0,0,0,0,0,0,0],\n",
    "                           #[-0.0625, -0.125, 0.25, 0,0,0,0,0,0,0,0],\n",
    "                           #[0,0,0, 0.25, -0.125, 0,0,0,0,0,0],\n",
    "                           #[0,0,0, -0.125, 0.25, 0,0,0,0,0,0],\n",
    "                           #[0,0,0,0,0, 0.25, -0.125,0,0,0,0],\n",
    "                           #[0,0,0,0,0, -0.125, 0.25,0,0,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0.25,0,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0.25,0,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0,0.25,0],\n",
    "                           #[0,0,0,0,0,0,0,0,0,0,0.25]])\n",
    "group_pref_var = 1.0*np.identity(11)\n",
    "\n",
    "for i in range(85):\n",
    "    subject_i_partworth = rng.multivariate_normal(group_pref_mean,group_pref_var)\n",
    "    for j in range(12):\n",
    "        x_j = prior_design_data.iloc[2*j + 24*i,5:16].to_numpy()\n",
    "        y_j = prior_design_data.iloc[2*j + 24*i + 1, 5:16].to_numpy()\n",
    "        #print(x_j)\n",
    "        #print(y_j)\n",
    "        pref_i = simulated_decision_maker_selection(subject_i_partworth,x_j,y_j)[0]\n",
    "        #print(pref_i)\n",
    "        if np.array_equal(x_j,pref_i):\n",
    "            prior_design_data.iloc[2*j + 24*i,2] = int(1)\n",
    "            prior_design_data.iloc[2*j + 24*i + 1, 2] = int(0)\n",
    "        else:\n",
    "            prior_design_data.iloc[2*j + 24*i,2] = int(0)\n",
    "            prior_design_data.iloc[2*j + 24*i + 1, 2] = int(1)\n",
    "\n",
    "        \n",
    "print(prior_design_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a8251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model specification for the screening study\n",
    "v_spec = OrderedDict()\n",
    "v_names = OrderedDict()\n",
    "\n",
    "for col, display_name in [(\"Color_Metallic\", \"Color_Metallic\"),(\"Color_White\",\"Color_White\"),(\"Color_Gray\",\"Color_Gray\"),\n",
    "                         (\"Wheels_Factory2\",\"Wheels_Factory2\"),(\"Wheels_Factory3\",\"Wheels_Factory3\"),\n",
    "                         (\"FB_Factory2\",\"FB_Factory2\"),(\"FB_Factory3\",\"FB_Factory3\"),(\"RB_Custom\",\"RB_Custom\"),(\"HL_Factory2\",\"HL_Factory2\"),\n",
    "                         (\"TL_Factory2\",\"TL_Factory2\"),(\"SM_Custom\",\"SM_Custom\")]:\n",
    "    v_spec[col] = [[1,2]]\n",
    "    v_names[col] = [display_name]\n",
    "\n",
    "    \n",
    "# List the variables that are the index variables (NEW 8/14/2024)\n",
    "index_var_names = [\"Color_Metallic\", \"Color_White\", \"Color_Gray\", \"Wheels_Factory2\", \"Wheels_Factory3\", \"FB_Factory2\",\n",
    "                  \"FB_Factory3\",\"RB_Custom\",\"HL_Factory2\",\"TL_Factory2\",\"SM_Custom\"]\n",
    "\n",
    "# Transform all of the index variable columns to have float dtypes\n",
    "for col in index_var_names:\n",
    "    prior_design_data[col] = prior_design_data[col].astype(float)\n",
    "\n",
    "#Test this out.\n",
    "#prior_design_data_limited = prior_design_data[prior_design_data[\"id\"] <= 10]\n",
    "\n",
    "#COMMENT THIS OUT 8/14/2024\n",
    "#v_model = pl.create_choice_model(data = prior_design_data, #prior_design_data_limited,#test this out.\n",
    "                                #alt_id_col = \"alt_id\",\n",
    "                                #obs_id_col = \"choice_situation\",\n",
    "                                #choice_col = \"choice\",\n",
    "                                #specification = v_spec,\n",
    "                                #model_type = \"MNL\",\n",
    "                                #names = v_names,\n",
    "                                #)\n",
    "\n",
    "v_model_mixed = pl.create_choice_model(data = prior_design_data,\n",
    "                                      alt_id_col = \"alt_id\",\n",
    "                                      obs_id_col = \"choice_situation\",\n",
    "                                      choice_col = \"choice\",\n",
    "                                      specification = v_spec,\n",
    "                                      model_type = \"Mixed Logit\",\n",
    "                                      names = v_names,\n",
    "                                      mixing_id_col = \"id\",\n",
    "                                      mixing_vars = index_var_names)\n",
    "\n",
    "#COMMENT THIS OUT 8/14/2024\n",
    "#We will save the utility coefficient estimates and variance.\n",
    "#v_model_fit = v_model.fit_mle(np.zeros(11), just_point = True)\n",
    "\n",
    "v_model_mixed_fit = v_model_mixed.fit_mle(init_vals=np.zeros(2 * len(index_var_names)),\n",
    "                      num_draws=600,\n",
    "                      seed=123, just_point = True)\n",
    "\n",
    "# Look at the estimated results\n",
    "#v_model_mixed.get_statsmodels_summary()\n",
    "\n",
    "#MAY CHANGE 8/14/2024\n",
    "#Save partworths and covariance matrix. We scale the covariance by 85 since we had 85 participants.\n",
    "#v_model_partworths = np.array(v_model_fit[\"x\"])\n",
    "#v_model_covariance = 85*np.array(v_model_fit[\"hess_inv\"])\n",
    "\n",
    "v_model_mixed_estimates = np.array(v_model_mixed_fit[\"x\"])\n",
    "\n",
    "#print('v_model_partworths: ' + str(v_model_partworths))\n",
    "#print('v_model_covariance: ' + str(v_model_covariance))\n",
    "#print('v_model_covariance scaled by number of participants: ' + str(85.0 * v_model_covariance))\n",
    "\n",
    "print(v_model_mixed_estimates)\n",
    "\n",
    "v_model_partworths = v_model_mixed_estimates[0:11]\n",
    "v_model_covariance = np.diag(np.square(v_model_mixed_estimates[11:22]))\n",
    "\n",
    "print(v_model_partworths)\n",
    "print(v_model_covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6b937",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the summary just to check the model.\n",
    "v_model_mixed.fit_mle(init_vals=np.zeros(2 * len(index_var_names)),\n",
    "                      num_draws=600,\n",
    "                      seed=123)\n",
    "v_model_mixed.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function creates a list of difference in products which satisfy dummy coding constraints for a specified list of \n",
    "#levels\n",
    "def product_diff_list_casestudy(levels):\n",
    "    ##levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    \n",
    "    #calculate the number of binary variables in the attribute vector.\n",
    "    num_binary_var = sum([lev - 1 for lev in levels])\n",
    "    \n",
    "    #make a call to product_diff_list to get a larger set of questions (difference between two alternatives) which violates the dummy coding constraints. We\n",
    "    #will filter out questions which violate the constraints.\n",
    "    question_list = product_diff_list(num_binary_var)\n",
    "    \n",
    "    #Create a list of questions which satisfy the dummy coding constraints\n",
    "    filtered_question_list = []\n",
    "    \n",
    "    \n",
    "    num_levels = len(levels)\n",
    "    \n",
    "    #For each question, we check whether it violates the dummy coding. If not, it gets added to our list of \n",
    "    #filtered questions.\n",
    "    for question in question_list:\n",
    "        lower_bound = 0\n",
    "        upper_bound = levels[0]-1\n",
    "        num_violations = 0\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            abs_sum_lev_question = sum([abs(question[j]) for j in range(lower_bound, upper_bound)])\n",
    "            if abs_sum_lev_question > 2:\n",
    "                num_violations = num_violations + 1\n",
    "            #Update lower and upper bound to go to the next level.\n",
    "            lower_bound = upper_bound\n",
    "            if i < num_levels-1:\n",
    "                upper_bound = upper_bound + (levels[i+1]-1)\n",
    "        \n",
    "        if num_violations == 0:\n",
    "            filtered_question_list.append(question)\n",
    "    \n",
    "    #The filtering above leaves questions which can have two levels of a factor activated at once. For example,\n",
    "    #for a 3x2 design the above filtering leaves the question [1 1 -1], which is not possible under dummy coding.\n",
    "    #We must filter out questions whose levels sum exactly to 2 (and also -2).\n",
    "    further_filtered_question_list = []\n",
    "    \n",
    "    for question in filtered_question_list:\n",
    "        lower_bound = 0\n",
    "        upper_bound = levels[0]-1\n",
    "        num_violations = 0\n",
    "        \n",
    "        for i in range(num_levels):\n",
    "            sum_lev_question = sum([question[j] for j in range(lower_bound, upper_bound)])\n",
    "            if sum_lev_question == 2 or sum_lev_question == -2:\n",
    "                num_violations = num_violations + 1\n",
    "            #Update lower and upper bound to go to the next level.\n",
    "            lower_bound = upper_bound\n",
    "            if i < num_levels-1:\n",
    "                upper_bound = upper_bound + (levels[i+1]-1)\n",
    "        \n",
    "        if num_violations == 0:\n",
    "            further_filtered_question_list.append(question)\n",
    "    \n",
    "    return further_filtered_question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d8d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the product_diff_list_casestudy function above.\n",
    "#print(product_diff_list_casestudy([3,3,2,2,2,2,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function is used to generate data to estimate the parameters in the normalized AO model. The normalized AO model\n",
    "#is given by log(D-err/Det^(1/2)(Sig)) ~ AM/||L*mu|| + AV/||S*Sig|| + AO/||S*Sig|| + ||L*mu|| + ||S*Sig||. AM, AV, and AO denote\n",
    "#the average question mean, average quesiton variance, and average question orthogonality of a given design under prior\n",
    "#N(mu, Sig). L and S denote varying signal and noise levels, respectively. The normalized AO model is used in our\n",
    "#optimization procedure so that we will not have to refit the parameters in the optimization model everytime the user\n",
    "#answers a batch. The idea is that varying L and S enough should encompass a wide enough range so that mu and Sig will\n",
    "#be within this range after updating.\n",
    "#In this function, we also include MO so that we may fit a maximum orthogonality model.\n",
    "\n",
    "#!!! rng will need to be set before calling this function !!!\n",
    "\n",
    "def norm_AO_MO_data_generation_casestudy(init_mu, init_Sig, batch_size, L, S, num_random_batches, num_true_partworths,rng,levels):\n",
    "    #init_mu: This is the initial expectation of the partworths. Should be a numpy array.\n",
    "    \n",
    "    #init_Sig: This is the initial covariance matrix of the partworths. Should be a square two-dimensional numpy array\n",
    "    #having rows and columns with same number of entries corresponding to init_mu.\n",
    "    \n",
    "    #batch_size: This is the number of questions in each batch. Should be an integer greater than or equal to one.\n",
    "    \n",
    "    #L: This is a vector which holds varying levels of signal (multiply with mu). For example,\n",
    "    #we could have L = [0.25,1.0,4.0]\n",
    "    \n",
    "    #S: This is a vector which holds varying levels of noise (multiply with Sig). For example,\n",
    "    #we could have S = [0.25,1.0,4.0]\n",
    "    \n",
    "    #num_random_batches: This is the number of random batches that we will generate for collecting data on log(D-err),\n",
    "    #AM, AV, and AO (and MO). This set of random batches will be used for each level combination of L and S. Should be an integer\n",
    "    #greater than or equal to one.\n",
    "    \n",
    "    #num_true_partworths: This is the number of true/baseline partworths we will use to evaluate the d-error of a design. Should be\n",
    "    #an integer greater than or equal to one.\n",
    "    \n",
    "    #rng: random number generator for generating multivariate normal vectors. Should be of the form rng = np.random.default_rng(seed)\n",
    "    #for some seed value.\n",
    "    \n",
    "    #levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    \n",
    "    attr_num = len(init_mu)\n",
    "    \n",
    "    #Create lists to store average orthogonality and max orthogonality, as well as d-error and average question mean and\n",
    "    #average question variance, and ||L*mu|| and ||S*Sig|| as well.\n",
    "    average_orthogonality = []\n",
    "    \n",
    "    maximum_orthogonality = []\n",
    "    \n",
    "    average_question_mean = []\n",
    "    \n",
    "    average_question_variance = []\n",
    "    \n",
    "    average_d_error = []\n",
    "    \n",
    "    L_mu = []\n",
    "    \n",
    "    S_Sig = []\n",
    "    \n",
    "    init_sqrt_determinant = []\n",
    "    \n",
    "    #Create a list of all products. WE USE product_diff_list_casestudy here instead of product_diff_list.\n",
    "    #prod_list = product_diff_list(attr_num)\n",
    "    prod_list = product_diff_list_casestudy(levels)\n",
    "    \n",
    "    #Construct the set of batch designs\n",
    "    batch_set = [[] for i in range(num_random_batches)]\n",
    "    for i in range(num_random_batches):\n",
    "        random_question_matrix = random.sample(prod_list,batch_size)\n",
    "        for m in range(batch_size):\n",
    "            #random_question_vec = random.sample(prod_list,1)[0]\n",
    "            #[x,y] = question_extractor(random_question_vec)\n",
    "            [x,y] = question_extractor(random_question_matrix[m])\n",
    "            batch_set[i].append([x,y])\n",
    "    \n",
    "    #Record the scaled norm of mu and Sig for each combination of L and S\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                L_mu.append(l*np.linalg.norm(init_mu,2))\n",
    "                S_Sig.append(s*np.linalg.norm(init_Sig,2))\n",
    "                init_sqrt_determinant.append(np.sqrt(np.linalg.det(s*init_Sig)))\n",
    "    \n",
    "    #Calculate AM, AV, AO and MO for each of the batches\n",
    "    for l in L:\n",
    "        for s in S:\n",
    "            for i in range(num_random_batches):\n",
    "                random_batch_question_mean = []\n",
    "                random_batch_question_variance = []\n",
    "                random_batch_orthogonality = []\n",
    "        \n",
    "                for p in range(batch_size):\n",
    "                    x_p = np.array(batch_set[i][p][0])\n",
    "                    y_p = np.array(batch_set[i][p][1])\n",
    "                    random_batch_question_mean.append(np.abs(np.dot(l*init_mu,x_p - y_p)))\n",
    "                    random_batch_question_variance.append(np.dot(x_p - y_p, np.dot(s*init_Sig,x_p - y_p)))\n",
    "                    for q in range(p+1, batch_size):\n",
    "                        x_q = np.array(batch_set[i][q][0])\n",
    "                        y_q = np.array(batch_set[i][q][1])\n",
    "                        random_batch_orthogonality.append(np.abs(np.dot(x_p - y_p, np.dot(s*init_Sig,x_q - y_q))))\n",
    "                \n",
    "                #We use this if statement in case the batch size is 1 because\n",
    "                #if the batch size is 1 then there are no orthogonality terms.\n",
    "                if len(random_batch_orthogonality) > 0:\n",
    "                    average_orthogonality.append(np.mean(np.array(random_batch_orthogonality)))\n",
    "                    maximum_orthogonality.append(np.max(np.array(random_batch_orthogonality)))\n",
    "        \n",
    "                average_question_mean.append(np.mean(np.array(random_batch_question_mean)))\n",
    "                average_question_variance.append(np.mean(np.array(random_batch_question_variance)))\n",
    "            \n",
    "    #Calculate the D-error.\n",
    "    for l in L:\n",
    "        print('L: '+str(l))\n",
    "        for s in S:\n",
    "            print('S: '+str(s))\n",
    "            true_partworths = []\n",
    "            for t in range(num_true_partworths):\n",
    "                #This is where rng needs to be set beforehand!\n",
    "                true_partworths.append(rng.multivariate_normal(l*init_mu,s*init_Sig))\n",
    "                \n",
    "            gumbel_errors = [[[np.random.gumbel(0,1) for k in range(2)] for j in range(batch_size)] for i in range(num_true_partworths)]\n",
    "            \n",
    "            for i in range(num_random_batches):\n",
    "                #Create a list for the batch that will store the final determinant value for each simulation\n",
    "                #corresponding to each baseline partworth.\n",
    "                batch_simulate_d_values = []\n",
    "                \n",
    "                #Simulate d-efficiency over baseline partworths\n",
    "                for j in range(len(true_partworths)):\n",
    "                #Each time we start with a new partworth, we must use the initial prior parameters.\n",
    "                    mu = l*init_mu\n",
    "                    Sig = s*init_Sig\n",
    "                    \n",
    "                    #Each simulation goes through the questions in the random batch.\n",
    "                    for k in range(batch_size):\n",
    "                    #Set x and y\n",
    "                        x = batch_set[i][k][0]\n",
    "                        y = batch_set[i][k][1]\n",
    "                \n",
    "                        #These temp variables will be used in the choice model below in case the user prefers y over x.\n",
    "                        x_temp = x\n",
    "                        y_temp = y\n",
    "                        \n",
    "                        gum_x = gumbel_errors[j][k][0]\n",
    "                        gum_y = gumbel_errors[j][k][1]\n",
    "                        #See preference between two products\n",
    "                        if (np.dot(true_partworths[j],np.array(y)) + gum_y) >= (np.dot(true_partworths[j],np.array(x)) + gum_x):\n",
    "                            x = y_temp\n",
    "                            y = x_temp\n",
    "                            \n",
    "                        #Perform moment matching after choice is made.\n",
    "                        [mu, Sig] = moment_matching_update(x,y,mu,Sig)\n",
    "                        \n",
    "                    #After the questionnaire for a baseline partworth is complete, we append the square root of the determinant\n",
    "                    #of the final covariance matrix.\n",
    "                    batch_simulate_d_values.append(np.sqrt(np.linalg.det(Sig)))\n",
    "                    \n",
    "                #We average the d-values from the simulation for a batch and store it in a list. This is the D-error of the batch i\n",
    "                #under distribution N(L*mu, S*Sig).\n",
    "                average_d_error.append(np.mean(batch_simulate_d_values))\n",
    "                \n",
    "    return average_orthogonality, maximum_orthogonality, average_question_mean, average_question_variance, L_mu, S_Sig, init_sqrt_determinant, average_d_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function constructs a batch design based off of average question mean, average question variance, and average\n",
    "#question orthogonality. For the average question orthogonality, we take the absolute value of the summands rather than\n",
    "#the square. We also normalize mu and Sig in the objective so that we do not need to keep on refitting the parameters \n",
    "#that go with question mean, question variance, and question orthogonality.\n",
    "\n",
    "#For the case study, we add functionality which allows constraints to enforce dummy coding for attributes which have more \n",
    "#than two levels.\n",
    "\n",
    "def batch_design_AO_casestudy(mu,Sig,batch_size,quest_mean_log_coeff,quest_var_log_coeff,quest_orth_log_coeff,t_lim = 100,logfile=False, levels = []):\n",
    "    #mu: expectation of prior on the DM's partworth. Should be a numpy array.\n",
    "    \n",
    "    #Sig: Covariance matrix of prior on the DM's partworth.  Should be a square two-dimensional numpy array\n",
    "    #having rows and columns with same number of entries corresponding to mu.\n",
    "    \n",
    "    \n",
    "    #batch_size: the number of questions we want to return in our batch design. This should be less or equal to the number\n",
    "    #of attributes (length of mu).\n",
    "    \n",
    "    #quest_mean_log_coeff: this is a fitting parameter that goes with the average question mean and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AM/||l*mu||.\n",
    "    \n",
    "    #quest_var_log_coeff: this is a fitting parameter that goes with the average question variance and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AV/||s*Sig||.\n",
    "    \n",
    "    #quest_orth_log_coeff: this is a fitting parameter that goes with the average question orthogonality and is obtained \n",
    "    #by fitting a linear model log (D-err/Init_det) ~ AM/||l*mu|| + AV/||s*Sig|| + AO/||s*Sig|| + ||l*mu|| + ||s*Sig|| and using the fitted parameter that goes with\n",
    "    #AO/||s*Sig||.\n",
    "    \n",
    "    #In the above three comments regarding the coefficients, (l,s) are scaling parameters for mu and Sig that divide the space into \n",
    "    #different signal-to-noise ratio regions.\n",
    "    \n",
    "    #t_lim: this is the max amount of time we want to take to construct the batch\n",
    "    #logfile: determine whether to print out a logfile of the optimization procedure.\n",
    "    \n",
    "    #levels: This is an array of numerical values, where the i-th entry denotes the number of levels of the i-th attribute.\n",
    "    #This will be used to enforce dummy coding constraints when there are more than two levels. The default value is []. In\n",
    "    #the default case, it is assumed all attributes have two levels. It should be the case that (outside the default\n",
    "    #case) the sum of (each entry\n",
    "    #in levels - 1) should equal the length of mu, due to the dummy coding conventions.\n",
    "\n",
    "    #Make sure that quest_orth_log_coeff is greater or equal to zero. Otherwise, we will\n",
    "    #have an unbounded optimization problem. In most situations, the fitting procedure\n",
    "    #will result in a positive value for quest_orth_log_coeff, but very rarely the fitting\n",
    "    #procedure will give a statistically non-significant but negative value for\n",
    "    #quest_orth_log_coeff that makes the optimization problem unbounded. When the quest_orth_log_coeff\n",
    "    #is less than 0, we decide to set it equal to 0. This will result in a bounded optimization problem,\n",
    "    #but the quality of the solution in terms of D-error may not be sufficient because we are no\n",
    "    #longer controlling orthogonality in the objective function.\n",
    "    if quest_orth_log_coeff<0.0:\n",
    "        quest_orth_log_coeff = 0.0\n",
    "\n",
    "    # This is the number of attributes for the products\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('Timelimit',t_lim)\n",
    "    if logfile:\n",
    "        m.setParam('LogFile',\"Batch_AO_batchsize\"+str(batch_size)+\"_meancoeff_\"+str(quest_mean_log_coeff)+\"_varcoeff_\"+\n",
    "               str(quest_var_log_coeff)+\"_orthcoeff_\"+str(quest_orth_log_coeff)+\"_v5.txt\")\n",
    "    \n",
    "    #calculate 2-norms of mu and Sigma\n",
    "    mu_2norm = np.linalg.norm(mu,2)\n",
    "    Sig_2norm = np.linalg.norm(Sig,2)\n",
    "    \n",
    "    #List of tuples for delta variable\n",
    "    if batch_size > 1:\n",
    "        delta_tuples = []\n",
    "        for i in range(batch_size):\n",
    "            for j in range(i+1,batch_size):\n",
    "                delta_tuples.append((i,j))\n",
    "    \n",
    "    #Set up the x_i and y_i, i = 1,...,batchsize\n",
    "    X = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    Y = m.addMVar((batch_size,n),vtype = GRB.BINARY)\n",
    "    if batch_size > 1:\n",
    "        Delta = m.addVars(delta_tuples, lb=0.0, vtype = GRB.CONTINUOUS)\n",
    "    \n",
    "    #Set up the objective function.\n",
    "    if batch_size > 1:\n",
    "        m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) + \n",
    "                       (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                       Y[i]@Sig@Y[i] for i in range(batch_size)]) + \n",
    "                           (quest_orth_log_coeff/(batch_size*(batch_size-1)*Sig_2norm/2))*sum([Delta[i,j] for i in range(batch_size) for j in range(i+1,batch_size)]),GRB.MINIMIZE)\n",
    "        \n",
    "    if batch_size == 1:\n",
    "        m.setObjective((quest_mean_log_coeff/(batch_size*mu_2norm))*sum([mu@X[i] - mu@Y[i] for i in range(batch_size)]) + \n",
    "                       (quest_var_log_coeff/(batch_size*Sig_2norm))*sum([X[i]@Sig@X[i] - X[i]@(2.0*Sig)@Y[i] + \n",
    "                       Y[i]@Sig@Y[i] for i in range(batch_size)]),GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up the constraints that force the products in question i to be different, as well as forcing the symmetry\n",
    "    #exploitation condition.\n",
    "    for i in range(batch_size):\n",
    "        m.addConstr(X[i]@X[i] - X[i]@Y[i] - Y[i]@X[i] + Y[i]@Y[i] >= 1)\n",
    "        m.addConstr(mu@X[i] - mu@Y[i] >= 0)\n",
    "        \n",
    "    #Set up the Sigma-orthogonality constraint for all questions i and j, i not equal to j. Also add constraints\n",
    "    #to make sure that questions within a batch are different, including with respect to switching order of products in\n",
    "    #the questions.\n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1,batch_size):\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] - Delta[i,j] <= 0)\n",
    "            m.addConstr(X[i]@Sig@X[j] - X[i]@Sig@Y[j] - Y[i]@Sig@X[j] + Y[i]@Sig@Y[j] + Delta[i,j] >= 0)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@X[j] + X[i]@Y[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@X[j] - Y[i]@Y[j] -\n",
    "                       X[j]@X[i] + X[j]@Y[i] + X[j]@X[j] - X[j]@Y[j] +\n",
    "                       Y[j]@X[i] - Y[j]@Y[i] - Y[j]@X[j] + Y[j]@Y[j] >= 1)\n",
    "            m.addConstr(X[i]@X[i] - X[i]@Y[i] - X[i]@Y[j] + X[i]@X[j] -\n",
    "                       Y[i]@X[i] + Y[i]@Y[i] + Y[i]@Y[j] - Y[i]@X[j] -\n",
    "                       Y[j]@X[i] + Y[j]@Y[i] + Y[j]@Y[j] - Y[j]@X[j] +\n",
    "                       X[j]@X[i] - X[j]@Y[i] - X[j]@Y[j] + X[j]@X[j] >= 1)\n",
    "            \n",
    "    #We add the dummy coding constraints if there are attributes with more than two levels. (7/27/2024)\n",
    "    num_attributes = len(levels)\n",
    "    if num_attributes > 0:\n",
    "        for i in range(batch_size):\n",
    "            index_level_tracker = 0\n",
    "            for j in range(num_attributes):\n",
    "                m.addConstr(sum([X[i,k] for k in range(index_level_tracker,index_level_tracker + levels[j]-1)]) <= 1)\n",
    "                m.addConstr(sum([Y[i,k] for k in range(index_level_tracker,index_level_tracker + levels[j]-1)]) <= 1)\n",
    "                index_level_tracker = index_level_tracker + (levels[j]-1)\n",
    "                #print(index_level_tracker)\n",
    "                \n",
    "            \n",
    "    m.optimize()\n",
    "    \n",
    "    #This will be the list of products\n",
    "    Q = [ [] for i in range(batch_size)]\n",
    "    D = [ [] for i in range(batch_size-1)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        Q[i].append(X[i].X)\n",
    "        Q[i].append(Y[i].X)\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        for j in range(i+1, batch_size):\n",
    "            D[i].append(Delta[i,j].X)\n",
    "        \n",
    "    return[Q,D]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93133b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the batch_design_AO_casestudy function above.\n",
    "#mu_test = np.ones(7)\n",
    "#Sig_test = np.identity(7)\n",
    "#batch_test = 4\n",
    "#mean_coeff_test = 0.03\n",
    "#var_coeff_test = -0.01\n",
    "#orth_coeff_test = 0.005\n",
    "#levels_test = [4,3,2,2]\n",
    "#print(batch_design_AO_casestudy(mu_test,Sig_test,batch_test,mean_coeff_test,var_coeff_test,orth_coeff_test,levels = levels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3aa41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our targeted decision maker.\n",
    "rng2 = np.random.default_rng(1000)\n",
    "target_dm_partworth = rng2.multivariate_normal(group_pref_mean,group_pref_var)\n",
    "print('target_dm_partworth: ' + str(target_dm_partworth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we start the process of learning the parameters for the MIP-AC (batch_design_AO_casestudy) model.\n",
    "rng3 = np.random.default_rng(101)\n",
    "\n",
    "\n",
    "batch_size_fit = 5\n",
    "\n",
    "#These are the vectors L and S discussed in \"Offline Learning Framework for Specifying MIP Objective Parameters\"\n",
    "L_fit = [0.5,1.0,2.0]\n",
    "S_fit = [0.5,1.0,2.0]\n",
    "\n",
    "#num_random_batches_fit is the number of batches for which we evaluate D-error, average question mean,\n",
    "#average question variance, average & maximum question covariance (orthogonality). num_true_partworths_fit is \n",
    "#the number of partworths used in simulating/evaluating D-error of each design.\n",
    "num_random_batches_fit = 1000\n",
    "num_true_partworths_fit = 50\n",
    "\n",
    "v_levels = [4,3,3,2,2,2,2]\n",
    "\n",
    "#Generate the data in order to estimate the parameters of the AO and MO models\n",
    "average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit = norm_AO_MO_data_generation_casestudy(v_model_partworths, v_model_covariance, batch_size_fit, L_fit, S_fit, num_random_batches_fit, num_true_partworths_fit,rng3,v_levels)\n",
    "\n",
    "#Create a dataframe of the generated data for fitting the parameters of the AO(MIP-AC) and MO(MIP-MC) models\n",
    "df_fit = pd.DataFrame(list(zip(average_orthogonality_fit, maximum_orthogonality_fit, average_question_mean_fit, average_question_variance_fit, L_mu_fit, S_Sig_fit, init_sqrt_determinant_fit, average_d_error_fit)),\n",
    "                  columns =['Avg_Orth', 'Max_Orth', 'Avg_Quest_Mean', 'Avg_Quest_Var', 'L_mu_norm', 'S_Sig_norm', 'Init_Sqrt_Det', 'D_err'])\n",
    "\n",
    "#Add some new columns to the dataset. We mean-center the independent variables to attempt to reduce VIF. This will not affect the value of\n",
    "#of the coefficients, except for the intercept. The intercept is not important because it is constant and thus will not be used\n",
    "#in the optimization problem.\n",
    "df_fit['log_norm_derr'] = np.log(np.divide(np.array(df_fit['D_err']),np.array(df_fit['Init_Sqrt_Det'])))\n",
    "df_fit['cent_norm_AM'] = np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Mean']),np.array(df_fit['L_mu_norm'])))\n",
    "df_fit['cent_norm_AV'] = np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Quest_Var']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_AO'] = np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Avg_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "df_fit['cent_norm_MO'] = np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])) - np.mean(np.divide(np.array(df_fit['Max_Orth']),np.array(df_fit['S_Sig_norm'])))\n",
    "\n",
    "df_fit['cent_L_mu_norm'] = df_fit['L_mu_norm'] - np.mean(np.array(df_fit['L_mu_norm']))\n",
    "df_fit['cent_S_Sig_norm'] = df_fit['S_Sig_norm'] - np.mean(np.array(df_fit['S_Sig_norm']))\n",
    "\n",
    "#Fitting the linear model for the AO (MIP-AC) model. Note that we do not use batch size\n",
    "#because it will be constant throughout the questionnaire.\n",
    "model_AO = sm.formula.ols(formula = \"log_norm_derr ~  cent_norm_AM + cent_norm_AV + cent_norm_AO + cent_L_mu_norm + cent_S_Sig_norm\", data = df_fit).fit()\n",
    "parameter_est_AO = model_AO.params\n",
    "\n",
    "print('parameter_est_AO: ' + str(parameter_est_AO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the questionnaire for the targeted decision maker.\n",
    "\n",
    "number_of_questions_target = 20\n",
    "batch_size_target = 5\n",
    "\n",
    "#Set the optimization model parameter estimates for MIP-AC\n",
    "AO_alpha_exp = parameter_est_AO[1]\n",
    "AO_kappa_exp = parameter_est_AO[2]\n",
    "AO_gamma_exp = parameter_est_AO[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f85e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start the questionnaire\n",
    "mu_target = v_model_partworths\n",
    "Sig_target = v_model_covariance\n",
    "\n",
    "for j in range(number_of_questions_target):\n",
    "    \n",
    "    if (j % batch_size_target == 0):\n",
    "        batch_AO = batch_design_AO_casestudy(mu_target,Sig_target,batch_size_target,AO_alpha_exp,AO_kappa_exp,AO_gamma_exp,t_lim = 50,levels = v_levels)[0]\n",
    "        print(batch_AO)\n",
    "\n",
    "    [x_target,y_target] = batch_AO[j % batch_size_target]\n",
    "    \n",
    "    [pref,not_pref] = simulated_decision_maker_selection(target_dm_partworth,x_target,y_target)\n",
    "    print([pref,not_pref])\n",
    "        \n",
    "    #Perform moment matching after choice is made.\n",
    "    [mu_target, Sig_target] = moment_matching_update(pref,not_pref,mu_target,Sig_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observe covariance and estimates of partworths after finishing the questionnaire. We observe the covariance in order\n",
    "#to get standard errors of the partworths.\n",
    "print('mu_target: ' + str(mu_target))\n",
    "print('Sig_target: ' + str(Sig_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fccfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare estimated ranking of top 5 vehicles for true partworth and estimated partworth\n",
    "\n",
    "#Create a new dataframe which has all the vehicle configurations. This dataframe comes from the dataset\n",
    "#Toyota_Corolla_Concept_Designs\n",
    "\n",
    "vehicle_concept_designs = pd.read_csv(\"Toyota_Corolla_Concept_Designs.csv\",header = 0)\n",
    "\n",
    "#We start by adding two columns to the dataframe, which are the utilities of each vehicle configuration under\n",
    "#the true partworth, and utilities under the estimated partworth.\n",
    "\n",
    "true_mean_utility = []\n",
    "est_mean_utility = []\n",
    "\n",
    "for i in range(len(vehicle_concept_designs)):\n",
    "    vehicle_i = vehicle_concept_designs.iloc[i,1:13].to_numpy()\n",
    "    #print(vehicle_i)\n",
    "    true_mean_utility.append(np.dot(target_dm_partworth,vehicle_i))\n",
    "    est_mean_utility.append(np.dot(mu_target,vehicle_i))\n",
    "\n",
    "print('true_mean_utility: ' + str(true_mean_utility))\n",
    "print('est_mean_utility: ' + str(est_mean_utility))\n",
    "\n",
    "vehicle_concept_designs[\"true_mean_util\"] = true_mean_utility\n",
    "vehicle_concept_designs[\"est_mean_util\"] = est_mean_utility\n",
    "\n",
    "vehicle_concept_designs_true_util = vehicle_concept_designs.sort_values(by = 'true_mean_util',ascending = False)\n",
    "\n",
    "print(vehicle_concept_designs_true_util.head())\n",
    "\n",
    "vehicle_concept_designs_est_util = vehicle_concept_designs.sort_values(by = 'est_mean_util', ascending = False)\n",
    "\n",
    "print(vehicle_concept_designs_est_util.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8090d3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate standard deviations and CI's\n",
    "standard_deviations_final = np.sqrt(np.diag(Sig_target))\n",
    "print(\"sd: \" + str(standard_deviations_final))\n",
    "\n",
    "lower_CI = mu_target - 1.96*standard_deviations_final\n",
    "upper_CI = mu_target + 1.96*standard_deviations_final\n",
    "\n",
    "print(\"Low CI: \" + str(lower_CI))\n",
    "print(\"Up CI: \" + str(upper_CI))\n",
    "\n",
    "cover = []\n",
    "for i in range(11):\n",
    "    if lower_CI[i] <= target_dm_partworth[i] and upper_CI[i] >= target_dm_partworth[i]:\n",
    "        cover.append(\"Cover\")\n",
    "    else:\n",
    "        cover.append(\"Not Cover\")\n",
    "        \n",
    "print(\"Cover: \" + str(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of utility values\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "sns.scatterplot(data=vehicle_concept_designs_est_util, x=\"est_mean_util\", y=\"true_mean_util\")\n",
    "\n",
    "plt.xlabel(\"Estimated Utility\")\n",
    "\n",
    "plt.ylabel(\"True Utility\")\n",
    "\n",
    "#ax.set(xlabel='Estimated Utility', ylabel='True Utility')\n",
    "\n",
    "fig.savefig('Correlation_True_Est_Utility.png',bbox_inches='tight')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MSEs\n",
    "mu_target_mse = np.mean(np.square(target_dm_partworth - mu_target))\n",
    "\n",
    "prior_mean_mse = np.mean(np.square(target_dm_partworth - v_model_partworths))\n",
    "\n",
    "print(\"mu_target_mse: \" + str(mu_target_mse))\n",
    "print(\"prior_mean_mse: \" + str(prior_mean_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e25ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate correlations\n",
    "np.corrcoef(vehicle_concept_designs_est_util[\"est_mean_util\"],vehicle_concept_designs_est_util[\"true_mean_util\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
