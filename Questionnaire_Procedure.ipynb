{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "611bbf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This module 'Questionnaire_Procedure' has all of the functions needed to do one-step and \n",
    "#two-step questionnaire, along with the two-step acquisition function since it depends on functions\n",
    "#coming from this module. It includes the following functions:\n",
    "#1. moment_matching_update\n",
    "#2. g_opt\n",
    "#3. two_stage_g_opt\n",
    "#4. two_step_g_acq (WE INCLUDE THIS HERE BECAUSE IT DEPENDS ON moment_matching_update AND g_opt, THEMATICALLY THIS\n",
    "#SHOULD NOT BE HERE)\n",
    "#JUST A SMALL CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca04f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import math\n",
    "import random # MARCH 3 2023\n",
    "\n",
    "import time\n",
    "import sklearn.datasets\n",
    "\n",
    "import sys\n",
    "sys.path.append(r'C:\\Users\\wsfishe\\Desktop\\PreferenceElicitationCode')\n",
    "from Baseline_Functions_Definitions import z_expectation_variance, g_fun #mu_log_coeff, Sig_log_coeff\n",
    "#Note that when we do 'from Baseline_Functions_Definitions import ...' we are importing:\n",
    "\n",
    "#FUNCTIONS:\n",
    "#1. z_expectation_variance\n",
    "#2. g_fun\n",
    "\n",
    "#!!NO LONGER USING!!\n",
    "#VARIABLES: (found from linear regression on log(g) in 'Baseline_Functions_Definitions')\n",
    "#mu_log_coeff = 0.03596804494858049\n",
    "#Sig_log_coeff = -0.020785433813507195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765e9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_matching_update(x,y,mu_prior,Sig_prior):\n",
    "    #x and y are a question pair, x is preferred over y.\n",
    "    #mu_prior and Sig_prior are expectation and covariance matrix\n",
    "    #Make sure x, y, mu_prior, and Sig_prior are numpy arrays\n",
    "    x_vec = np.array(x)\n",
    "    y_vec = np.array(y)\n",
    "    mu_prior_vec = np.array(mu_prior)\n",
    "    Sig_prior_vec = np.array(Sig_prior)\n",
    "    \n",
    "    #Define question expectation and question variance\n",
    "    v = x_vec - y_vec\n",
    "    mu_v = np.dot(mu_prior_vec,v)\n",
    "    Sig_dot_v = np.dot(Sig_prior_vec,v)\n",
    "    Sig_v = np.dot(v,Sig_dot_v)\n",
    "    \n",
    "    #Save np.dot(Sig_prior_vec,v) as a variable (DONE)\n",
    "    \n",
    "    #Calculate expectation and variance for Z random variable\n",
    "    \n",
    "    [mu_z, var_z] = z_expectation_variance(mu_v,Sig_v)\n",
    "    \n",
    "    \n",
    "    #Calculate the update expectation and covariance matrix for \n",
    "    #posterior\n",
    "    mu_posterior = mu_prior_vec + (mu_z/math.sqrt(Sig_v))*Sig_dot_v\n",
    "    Sig_posterior = ((var_z-1)/Sig_v)*np.outer(Sig_dot_v,Sig_dot_v) + Sig_prior_vec\n",
    "    \n",
    "    return mu_posterior, Sig_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6dd18d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example2: moment_matching_update\n",
    "#ex2_x = [1,1,0,1]\n",
    "#ex2_y = [0,0,1,1]\n",
    "#ex2_mu = [0,0,0,0]\n",
    "#ex2_Sig = [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n",
    "#print(moment_matching_update(ex2_x,ex2_y,ex2_mu,ex2_Sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed66e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate an optimization problem to get the optimal solution to the one-step lookahead problem\n",
    "\n",
    "def g_opt(mu, Sig, mu_log_coeff, Sig_log_coeff):\n",
    "    #mu: expectation of prior on beta\n",
    "    #Sig: covariance matrix of prior on beta\n",
    "    #mu_log_coeff: the estimated coefficient c_1 that goes with m in the linear model log(g) = c_1*m + c_2*v\n",
    "    #Sig_log_coeff: the estimated coefficient c_2 that goes with v in the linear model log(g) = c_1m + c_2*v\n",
    "    \n",
    "    #n is number of attributes\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    #Log coefficients:\n",
    "    mu_s = mu_log_coeff*mu \n",
    "    Sig_s = Sig_log_coeff*Sig \n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('OutputFlag', 0)\n",
    "    #m.setParam('MIPGap', 0)\n",
    "    #m.params.DualReductions = 0\n",
    "    \n",
    "    #Set up x and y binary vectors, other variables\n",
    "    x = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"x\")\n",
    "    y = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"y\")\n",
    "    \n",
    "    #Objective function, constant values obtained from R lm function, regression on log(g)\n",
    "    m.setObjective(mu_s@x - mu_s@y + x@Sig_s@x - y@Sig_s@x - x@Sig_s@y + y@Sig_s@y,\n",
    "                   GRB.MINIMIZE)\n",
    "\n",
    "    #Set up constraint so that x and y are different\n",
    "    m.addConstr(x@x - x@y - y@x + y@y >= 1)\n",
    "    \n",
    "    #We want mu(x-y) >= 0 due to the symmetry of the g function\n",
    "    m.addConstr(mu@x - mu@y >= 0)\n",
    "    \n",
    "    m.optimize()\n",
    "    \n",
    "    #Return solution x and y\n",
    "    Vars = m.getVars()\n",
    "    x_sol = []\n",
    "    y_sol = []\n",
    "    for u in range(0,n):\n",
    "        x_sol.append(Vars[u].x)\n",
    "        \n",
    "    for w in range(n,2*n):\n",
    "        y_sol.append(Vars[w].x)\n",
    "    \n",
    "    \n",
    "    return [m.objVal,x_sol,y_sol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ab69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate an optimization problem to get the optimal solution to the one-step lookahead problem when there are multiple\n",
    "#levels for each attribute. The products are not binarized before the preference learning procedure.\n",
    "\n",
    "#ADDED March 6, 2023\n",
    "\n",
    "def g_opt_multi_lvl(mu, Sig, mu_log_coeff, Sig_log_coeff, num_lvl_vec):\n",
    "    #mu: expectation of prior on beta\n",
    "    #Sig: covariance matrix of prior on beta\n",
    "    #mu_log_coeff: the estimated coefficient c_1 that goes with m in the linear model log(g) = c_1*m + c_2*v\n",
    "    #Sig_log_coeff: the estimated coefficient c_2 that goes with v in the linear model log(g) = c_1*m + c_2*v\n",
    "    #num_lvl_vec: this is a vector containing the number of levels for each attribute. For example, [4,3,5] denotes\n",
    "    #three attributes with 4, 3, and 5 levels respectively.\n",
    "    \n",
    "    #n is number of attributes\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    #Log coefficients:\n",
    "    mu_s = mu_log_coeff*mu \n",
    "    Sig_s = Sig_log_coeff*Sig \n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('OutputFlag', 0)\n",
    "    m.params.NonConvex = 0\n",
    "    \n",
    "    #This is the total number of binary variables we will have.\n",
    "    bin_var_len = sum([(math.floor(math.log2(L - 1)) + 1) for L in num_lvl_vec])\n",
    "    \n",
    "    #Write out the binary basis values for each attribute and save them in a list\n",
    "    basis = []\n",
    "    for i in range(n):\n",
    "        basis_i = [2**j  for j in range(math.floor(math.log2(num_lvl_vec[i] - 1)) + 1)]\n",
    "        basis.append(basis_i)\n",
    "        \n",
    "    \n",
    "    #Set up the binary encoding xb_i and yb_i, i = 1,...,n\n",
    "    xb = m.addMVar(shape = bin_var_len,vtype = GRB.BINARY)\n",
    "    yb = m.addMVar(shape = bin_var_len,vtype = GRB.BINARY)\n",
    "    \n",
    "    \n",
    "    #Objective function, constant values obtained from R lm function, regression on log(g)\n",
    "    #MARCH 14, 2023\n",
    "    m.setObjective(gp.quicksum([mu_s[i]*(gp.quicksum([basis[i][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) \n",
    "                                                                     for k in range(i)]) + j] for j in range(len(basis[i]))]) -\n",
    "                                    gp.quicksum([basis[i][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) \n",
    "                                                                     for k in range(i)]) + j] for j in range(len(basis[i]))])) for i in range(n)]) +\n",
    "                  gp.quicksum([gp.quicksum([(gp.quicksum([basis[s][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(s)]) + j] \n",
    "                                                          for j in range(len(basis[s]))])*gp.quicksum([basis[r][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(r)]) + j]\n",
    "                                                                                                       for j in range(len(basis[r]))]) +\n",
    "                                            gp.quicksum([basis[s][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(s)]) + j] \n",
    "                                                         for j in range(len(basis[s]))])*gp.quicksum([basis[r][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(r)]) + j] \n",
    "                                                                                                      for j in range(len(basis[r]))]) -\n",
    "                                            gp.quicksum([basis[s][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(s)]) + j] for j in range(len(basis[s]))])*\n",
    "                                            gp.quicksum([basis[r][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(r)]) + j] for j in range(len(basis[r]))]) -\n",
    "                                            gp.quicksum([basis[r][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(r)]) + j] for j in range(len(basis[r]))])*\n",
    "                                            gp.quicksum([basis[s][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(s)]) + j] for j in range(len(basis[s]))]))*Sig_s[s,r] \n",
    "                                            for s in range(n)]) for r in range(n)]), GRB.MINIMIZE)\n",
    "    \n",
    "    \n",
    "    #Constraint (C3)', ensures the products are different. At least one of the x components is different from y component.\n",
    "    m.addConstr(gp.quicksum([(xb[i]-yb[i])*(xb[i]-yb[i]) for i in range(bin_var_len)]) >= 1)\n",
    "    \n",
    "    \n",
    "    #We want mu(x-y) >= 0 due to the symmetry of the g function\n",
    "    #MARCH 14, 2023\n",
    "    m.addConstr(gp.quicksum([mu[i]*(gp.quicksum([basis[i][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) \n",
    "                                                                     for k in range(i)]) + j] for j in range(len(basis[i]))]) -\n",
    "                                    gp.quicksum([basis[i][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) \n",
    "                                                                     for k in range(i)]) + j] for j in range(len(basis[i]))])) for i in range(n)]) >= 0)\n",
    "    \n",
    "    \n",
    "    #Adding constraints (NEWC1)' and (NEWC2)'\n",
    "    #MARCH 14, 2023\n",
    "    m.addConstr(gp.quicksum( [basis[0][j]*xb[j] for j in range(len(basis[0]))] ) <= num_lvl_vec[0]-1)\n",
    "    m.addConstr(gp.quicksum( [basis[0][j]*yb[j] for j in range(len(basis[0]))] ) <= num_lvl_vec[0]-1)\n",
    "    \n",
    "    #(adding (NEWC1)' and (NEWC2)' continued)\n",
    "    #MARCH 14, 2023\n",
    "    for i in range(1,n):\n",
    "        m.addConstr(gp.quicksum([basis[i][j]*xb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(i)]) + j] for j in range(len(basis[i]))]) <= num_lvl_vec[i]-1)\n",
    "        m.addConstr(gp.quicksum([basis[i][j]*yb[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(i)]) + j] for j in range(len(basis[i]))]) <= num_lvl_vec[i]-1)\n",
    "    \n",
    "    m.optimize()\n",
    "    \n",
    "    #Convert the binary variables xb and yb back into multi-level quantitative attributes for x and y\n",
    "    x = np.array([sum(xb.X[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(i)]) + j]*basis[i][j] for j in range(len(basis[i]))) for i in range(n)])\n",
    "    y = np.array([sum(yb.X[sum([(math.floor(math.log2(num_lvl_vec[k] - 1)) + 1) for k in range(i)]) + j]*basis[i][j] for j in range(len(basis[i]))) for i in range(n)])\n",
    "    \n",
    "    \n",
    "    return [m.objVal,x,y,xb.X,yb.X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6403b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Formulate an optimization problem to get the optimal solution to the one-step lookahead problem when there are multiple\n",
    "#levels for each attribute. The products are binarized before the preference learning procedure.\n",
    "\n",
    "#ADDED March 7, 2023\n",
    "\n",
    "def g_opt_multi_lvl_v2(mu_b, Sig_b, mu_log_coeff, Sig_log_coeff, num_lvl_vec):\n",
    "    \n",
    "    #n is number of attributes\n",
    "    n = len(Sig_b[0])\n",
    "    \n",
    "    #Log coefficients:\n",
    "    mu_s = mu_log_coeff*mu_b\n",
    "    Sig_s = Sig_log_coeff*Sig_b \n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('OutputFlag', 0)\n",
    "    \n",
    "    #This is the total number of binary variables we will have.\n",
    "    bin_var_len = sum([(L - 1) for L in num_lvl_vec])\n",
    "    \n",
    "    #Set up the binary encoding xb_i and yb_i.\n",
    "    xb = m.addMVar(shape = bin_var_len,vtype = GRB.BINARY)\n",
    "    yb = m.addMVar(shape = bin_var_len,vtype = GRB.BINARY)\n",
    "    \n",
    "    #Objective function, constant values obtained from R lm function, regression on log(g)\n",
    "    m.setObjective(mu_s@xb - mu_s@yb + xb@Sig_s@xb - yb@Sig_s@xb - xb@Sig_s@yb + yb@Sig_s@yb,\n",
    "                   GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up constraint so that x and y are different\n",
    "    m.addConstr(xb@xb - xb@yb - yb@xb + yb@yb >= 1)\n",
    "    \n",
    "    #We want mu(x-y) >= 0 due to the symmetry of the g function\n",
    "    m.addConstr(mu_b@xb - mu_b@yb >= 0)\n",
    "    \n",
    "    #Add constraints (C3)' and (C5)'.\n",
    "    m.addConstr(gp.quicksum([xb[i] for i in range(num_lvl_vec[0]-1)]) <= 1)\n",
    "    m.addConstr(gp.quicksum([yb[i] for i in range(num_lvl_vec[0]-1)]) <= 1)\n",
    "    \n",
    "    for j in range(1,n-1):\n",
    "        m.addConstr(gp.quicksum([xb[i] for i in range(sum([(num_lvl_vec[k] - 1) for k in range(j)]) + 1,sum([(num_lvl_vec[k] - 1) for k in range(j+1)]))]) <= 1)\n",
    "        m.addConstr(gp.quicksum([yb[i] for i in range(sum([(num_lvl_vec[k] - 1) for k in range(j)]) + 1,sum([(num_lvl_vec[k] - 1) for k in range(j+1)]))]) <= 1)\n",
    "    m.optimize()\n",
    "    \n",
    "    return [m.objVal,xb.X,yb.X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2057c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will need to be moved!!!\n",
    "#Example 5: g_opt\n",
    "#rng = np.random.default_rng(100) \n",
    "#np.random.seed(100)\n",
    "#random.seed(100)\n",
    "\n",
    "#ex5_mu_1 = rng.uniform(low = -1.0, high = 1.0, size = 6)# np.array(6*[1.0])\n",
    "#ex5_Sig_1 = sklearn.datasets.make_spd_matrix(6)#np.identity(6)\n",
    "#ex5_mu_2 =  np.array([-0.125, 0.25, -0.375, 0.5, -0.625, 0.75, -0.875, 1.0, -1.125, 1.25, -1.375, 1.5])\n",
    "#ex5_Sig_2 = sklearn.datasets.make_spd_matrix(12)#np.identity(12)\n",
    "#ex5_mlc = 0.030273590906016633#for 6 attributes # 0.019304323842200457 #for 12 attributes\n",
    "#ex5_Slc = -0.006008375621810446#for 6 attributes # -0.0016111804008083416 #for 12 attributes\n",
    "#start_time_one_step_1 = time.perf_counter()\n",
    "#print(g_opt(ex5_mu_1,ex5_Sig_1,ex5_mlc,ex5_Slc))\n",
    "#print(time.perf_counter() - start_time_one_step_1, \"seconds\")\n",
    "\n",
    "#start_time_one_step_2 = time.perf_counter()\n",
    "#print(g_opt(ex5_mu_2,ex5_Sig_2,ex5_mlc,ex5_Slc))\n",
    "#print(time.perf_counter() - start_time_one_step_2, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a1de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WILL NEED TO BE MOVED!!!\n",
    "#Example 5a: g_opt_multi_lvl\n",
    "#rng = np.random.default_rng(10000) \n",
    "#np.random.seed(10000)\n",
    "#random.seed(10000)\n",
    "\n",
    "#ex5a_mu_1 = rng.uniform(low = -1.0, high = 1.0, size = 6)# np.array(6*[1.0])\n",
    "#ex5a_Sig_1 = sklearn.datasets.make_spd_matrix(6)#np.identity(6)\n",
    "#ex5a_num_lvl_vec = [3,3,3,3,3,3]\n",
    "#ex5_mu_2 =  np.array([-0.125, 0.25, -0.375, 0.5, -0.625, 0.75, -0.875, 1.0, -1.125, 1.25, -1.375, 1.5])\n",
    "#ex5_Sig_2 = sklearn.datasets.make_spd_matrix(12)#np.identity(12)\n",
    "#ex5a_mlc = 0.030273590906016633#for 6 attributes # 0.019304323842200457 #for 12 attributes\n",
    "#ex5a_Slc = -0.006008375621810446#for 6 attributes # -0.0016111804008083416 #for 12 attributes\n",
    "#ex5a_mlc_1 = 0.013840168431387311\n",
    "#ex5a_Slc_1 = -0.000737349763664796\n",
    "#start_time_one_step_1 = time.perf_counter()\n",
    "#print(g_opt_multi_lvl(ex5a_mu_1,ex5a_Sig_1,ex5a_mlc,ex5a_Slc,ex5a_num_lvl_vec))\n",
    "#print(time.perf_counter() - start_time_one_step_1, \"seconds\")\n",
    "\n",
    "#start_time_one_step_2 = time.perf_counter()\n",
    "#print(g_opt_multi_lvl(ex5a_mu_1,ex5a_Sig_1,ex5a_mlc_1,ex5a_Slc_1,ex5a_num_lvl_vec))\n",
    "#print(time.perf_counter() - start_time_one_step_2, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06d00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WILL NEED TO BE MOVED!!!\n",
    "#Example 5b: g_opt_multi_lvl_v2\n",
    "#rng = np.random.default_rng(1000) \n",
    "#np.random.seed(1000)\n",
    "#random.seed(1000)\n",
    "\n",
    "#ex5b_mu_1 = rng.uniform(low = -1.0, high = 1.0, size = 6)# np.array(6*[1.0])\n",
    "#ex5b_Sig_1 = sklearn.datasets.make_spd_matrix(6)#np.identity(6)\n",
    "#ex5b_num_lvl_vec = [2,2,2,2,2,2]\n",
    "#ex5_mu_2 =  np.array([-0.125, 0.25, -0.375, 0.5, -0.625, 0.75, -0.875, 1.0, -1.125, 1.25, -1.375, 1.5])\n",
    "#ex5_Sig_2 = sklearn.datasets.make_spd_matrix(12)#np.identity(12)\n",
    "#ex5b_mlc = 0.030273590906016633#for 6 attributes # 0.019304323842200457 #for 12 attributes\n",
    "#ex5b_Slc = -0.006008375621810446#for 6 attributes # -0.0016111804008083416 #for 12 attributes\n",
    "#start_time_one_step_1 = time.perf_counter()\n",
    "#print(g_opt_multi_lvl_v2(ex5b_mu_1,ex5b_Sig_1,ex5b_mlc,ex5b_Slc,ex5b_num_lvl_vec))\n",
    "#print(time.perf_counter() - start_time_one_step_1, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d9f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define optimization problem for approximate two-step acquisition, exploiting orthogonality property\n",
    "#|q_1 * Sig * q_0| < epsilon.\n",
    "\n",
    "#include time_limit as a parameter. Looking at attributes equal to 12, it appears 100 seconds is sufficient for most cases.\n",
    "def two_stage_g_opt(mu, Sig, mu_log_coeff, Sig_log_coeff, epsilon, t_lim = 100):\n",
    "    #mu: expectation of prior on beta\n",
    "    #Sig: Covariance matrix of prior on beta\n",
    "    #mu_log_coeff: the estimated coefficient c_1 that goes with m in the linear model log(g) = c_1*m + c_2*v\n",
    "    #Sig_log_coeff: the estimated coefficient c_2 that goes with v in the linear model log(g) = c_1m + c_2*v\n",
    "    #epsilon: bound for orthogonality contraint, should be small.\n",
    "    \n",
    "    #number of attributes\n",
    "    n = len(Sig[0])\n",
    "    \n",
    "    #Scale mu and Sig by the parameters we received from linear approximation\n",
    "    mu_s = mu_log_coeff*mu\n",
    "    Sig_s = Sig_log_coeff*Sig\n",
    "    \n",
    "    # Create a new model\n",
    "    m = gp.Model(\"mip1\")\n",
    "    m.setParam('Timelimit', t_lim)\n",
    "    #m.setParam('OutputFlag', 0)\n",
    "    \n",
    "    #Set up x_0, y_0, x_1, and y_1 binary vectors, other variables\n",
    "    x_0 = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"x_0\")\n",
    "    y_0 = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"y_0\")\n",
    "    x_1 = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"x_1\")\n",
    "    y_1 = m.addMVar(shape = n, vtype = GRB.BINARY, name = \"y_1\")\n",
    "    \n",
    "    #Objective function, coefficient values obtained from R lm function, regression on g\n",
    "    #for 0<=mu<=3 and sig>=2.5 \n",
    "    \n",
    "    m.setObjective(mu_s@x_0 - mu_s@y_0 + mu_s@x_1 - mu_s@y_1 + x_0@Sig_s@x_0 -x_0@(2.0*Sig_s)@y_0 + \n",
    "                   y_0@Sig_s@y_0 + x_1@Sig_s@x_1 - x_1@(2.0*Sig_s)@y_1 + y_1@Sig_s@y_1,\n",
    "                   GRB.MINIMIZE)\n",
    "    \n",
    "    #Set up constraint so that x_0 and y_0 are different\n",
    "    \n",
    "    m.addConstr(x_0@x_0 - x_0@y_0 - y_0@x_0 + y_0@y_0 >= 1)\n",
    "    \n",
    "    #Set up constraint so that x_1 and y_1 are different\n",
    "    \n",
    "    m.addConstr(x_1@x_1 - x_1@y_1 - y_1@x_1 + y_1@y_1 >= 1)\n",
    "    \n",
    "    #Set up mu_v_0, where v_0 = x_0-y_0. We want mu_v_0 >= 0 due to the symmetry of the g function\n",
    "\n",
    "    m.addConstr(mu@x_0 - mu@y_0 >= 0)\n",
    "    \n",
    "    #Set up mu_v_1, where v_1 = x_1-y_1. We want mu_v_1 >= 0 due to the symmetry of the g function\n",
    "\n",
    "    m.addConstr(mu@x_1 - mu@y_1 >= 0)\n",
    "    \n",
    "    #Set up orthogonality constraint\n",
    "    \n",
    "    m.addConstr(x_1@Sig@x_0 - x_1@Sig@y_0 - y_1@Sig@x_0 + y_1@Sig@y_0 <= epsilon)\n",
    "    m.addConstr(x_1@Sig@x_0 - x_1@Sig@y_0 - y_1@Sig@x_0 + y_1@Sig@y_0 >= -epsilon)\n",
    "    \n",
    "    m.optimize()\n",
    "    \n",
    "    #Return solutions x_0,y_0 and x_1,y_1\n",
    "    Vars = m.getVars()\n",
    "    x_0_sol = []\n",
    "    y_0_sol = []\n",
    "    x_1_sol = []\n",
    "    y_1_sol = []\n",
    "    for u in range(0,n):\n",
    "        x_0_sol.append(Vars[u].x)\n",
    "        \n",
    "    for w in range(n,2*n):\n",
    "        y_0_sol.append(Vars[w].x)\n",
    "    \n",
    "    for u in range(2*n,3*n):\n",
    "        x_1_sol.append(Vars[u].x)\n",
    "        \n",
    "    for w in range(3*n,4*n):\n",
    "        y_1_sol.append(Vars[w].x)\n",
    "    \n",
    "    \n",
    "    return [x_0_sol,y_0_sol,x_1_sol,y_1_sol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d82980a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example10: two_stage_g_opt\n",
    "#ex10_mu = np.array([1,0,0,0])\n",
    "#ex10_Sig = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "#ex10_mlc = 0.036\n",
    "#ex10_Slc = -0.021\n",
    "#ex10_eps = 0.1\n",
    "#print('two_stage_question_pair: ' + str(two_stage_g_opt(ex10_mu,ex10_Sig,ex10_mlc,ex10_Slc,ex10_eps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207abdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to evaluate the two_step value of a question given prior parameters mu_0 and Sig_0\n",
    "#x_0 and y_0 are a question pair.\n",
    "#This is the exact two-step g value.\n",
    "def two_step_g_acq(mu_0,Sig_0,mu_log_coeff,Sig_log_coeff,x_0,y_0):\n",
    "    #mu_0 and Sig_0: These are the parameters (expectation and covariance) of the prior distribution\n",
    "    #mu_log_coeff and Sig_log_coeff: These are parameters used in the linear model approximation log(g) = c_1*m + c_2*v\n",
    "    #x_0 and y_0: These are a question pair that we are interested in evaluating\n",
    "    \n",
    "    #Ensure that the given arguments are numpy arrays for processing below.\n",
    "    x_0_vec = np.array(x_0)\n",
    "    y_0_vec = np.array(y_0)\n",
    "    mu_0_vec = np.array(mu_0)\n",
    "    Sig_0_vec = np.array(Sig_0)\n",
    "    \n",
    "    #Define first stage variables\n",
    "    m_0 = np.dot(mu_0_vec,x_0_vec - y_0_vec)\n",
    "    v_0 = np.dot(x_0_vec-y_0_vec,np.dot(Sig_0_vec,x_0_vec-y_0_vec))\n",
    "    \n",
    "    #Gather the posterior information given the two scenarios where the individual picks x over y or they pick\n",
    "    #y over x\n",
    "    [mu_10,Sig_10] = moment_matching_update(x_0_vec,y_0_vec,mu_0_vec,Sig_0_vec)\n",
    "    [mu_11,Sig_11] = moment_matching_update(y_0_vec,x_0_vec,mu_0_vec,Sig_0_vec)\n",
    "    \n",
    "    #Solve g_opt(mu_10,Sig_10)[0] and g_opt(mu_11,Sig_11)[0] in order to get optimal questions for each scenario, where\n",
    "    #each scenario is the individual picking x or y.\n",
    "    \n",
    "    [x_10,y_10] = g_opt(mu_10,Sig_10,mu_log_coeff,Sig_log_coeff)[1:]\n",
    "    [x_11,y_11] = g_opt(mu_11,Sig_11,mu_log_coeff,Sig_log_coeff)[1:]\n",
    "    \n",
    "    #Define second stage variables.\n",
    "    m_10 = np.dot(np.array(mu_10),np.array(x_10) - np.array(y_10))\n",
    "    v_10 = np.dot(np.array(x_10) - np.array(y_10),np.dot(np.array(Sig_10),np.array(x_10)-np.array(y_10)))\n",
    "    m_11 = np.dot(np.array(mu_11),np.array(x_11) - np.array(y_11))\n",
    "    v_11 = np.dot(np.array(x_11) - np.array(y_11),np.dot(np.array(Sig_11),np.array(x_11)-np.array(y_11)))\n",
    "    \n",
    "    #Calculate the two-step value. fst_stg_g_sum_term are the two summation terms of g(m_0,v_0)\n",
    "    fst_stg_g_sum_term = g_fun(m_0,v_0)[1:]\n",
    "    two_step_g = g_fun(m_10,v_10)[0]*fst_stg_g_sum_term[0] + g_fun(m_11,v_11)[0]*fst_stg_g_sum_term[1]\n",
    "    \n",
    "    return [two_step_g,x_10,y_10,x_11,y_11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aa2808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to evaluate the two_step value of a question given prior parameters mu_0 and Sig_0\n",
    "#x_0 and y_0 are a question pair, where x_0 and y_0 have attributes with multiple quantitative levels.\n",
    "#This is the exact two-step g value.\n",
    "\n",
    "def multlvl_two_step_g_acq(mu_0,Sig_0,mu_log_coeff,Sig_log_coeff,x_0,y_0,num_lvl_vec):\n",
    "    #mu_0 and Sig_0: These are the parameters (expectation and covariance) of the prior distribution\n",
    "    #mu_log_coeff and Sig_log_coeff: These are parameters used in the linear model approximation log(g) = c_1*m + c_2*v\n",
    "    #x_0 and y_0: These are a question pair that we are interested in evaluating\n",
    "    #num_lvl_vec: this is a vector containing the number of levels for each attribute. For example, [4,3,5] denotes\n",
    "    #three attributes with 4, 3, and 5 levels respectively.\n",
    "    \n",
    "    #Ensure that the given arguments are numpy arrays for processing below.\n",
    "    x_0_vec = np.array(x_0)\n",
    "    y_0_vec = np.array(y_0)\n",
    "    mu_0_vec = np.array(mu_0)\n",
    "    Sig_0_vec = np.array(Sig_0)\n",
    "    \n",
    "    #Define first stage variables\n",
    "    m_0 = np.dot(mu_0_vec,x_0_vec - y_0_vec)\n",
    "    v_0 = np.dot(x_0_vec-y_0_vec,np.dot(Sig_0_vec,x_0_vec-y_0_vec))\n",
    "    \n",
    "    #Gather the posterior information given the two scenarios where the individual picks x over y or they pick\n",
    "    #y over x\n",
    "    [mu_10,Sig_10] = moment_matching_update(x_0_vec,y_0_vec,mu_0_vec,Sig_0_vec)\n",
    "    [mu_11,Sig_11] = moment_matching_update(y_0_vec,x_0_vec,mu_0_vec,Sig_0_vec)\n",
    "    \n",
    "    #Solve g_opt(mu_10,Sig_10)[0] and g_opt(mu_11,Sig_11)[0] in order to get optimal questions for each scenario, where\n",
    "    #each scenario is the individual picking x or y.\n",
    "    \n",
    "    [x_10,y_10] = g_opt_multi_lvl(mu_10,Sig_10,mu_log_coeff,Sig_log_coeff,num_lvl_vec)[1:3]\n",
    "    [x_11,y_11] = g_opt_multi_lvl(mu_11,Sig_11,mu_log_coeff,Sig_log_coeff,num_lvl_vec)[1:3]\n",
    "    \n",
    "    #Define second stage variables.\n",
    "    m_10 = np.dot(np.array(mu_10),np.array(x_10) - np.array(y_10))\n",
    "    v_10 = np.dot(np.array(x_10) - np.array(y_10),np.dot(np.array(Sig_10),np.array(x_10)-np.array(y_10)))\n",
    "    m_11 = np.dot(np.array(mu_11),np.array(x_11) - np.array(y_11))\n",
    "    v_11 = np.dot(np.array(x_11) - np.array(y_11),np.dot(np.array(Sig_11),np.array(x_11)-np.array(y_11)))\n",
    "    \n",
    "    #Calculate the two-step value. fst_stg_g_sum_term are the two summation terms of g(m_0,v_0)\n",
    "    fst_stg_g_sum_term = g_fun(m_0,v_0)[1:]\n",
    "    two_step_g = g_fun(m_10,v_10)[0]*fst_stg_g_sum_term[0] + g_fun(m_11,v_11)[0]*fst_stg_g_sum_term[1]\n",
    "    \n",
    "    return [two_step_g,x_10,y_10,x_11,y_11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abeb9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS NEEDS TO BE MOVED!!!\n",
    "#Example7: two_step_g_acq\n",
    "#rng = np.random.default_rng(100) \n",
    "#np.random.seed(100)\n",
    "#random.seed(100)\n",
    "\n",
    "#ex7_mu = rng.uniform(low = -1.0, high = 1.0, size = 6)\n",
    "#ex7_Sig = sklearn.datasets.make_spd_matrix(6)#[[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1]]\n",
    "#ex7_mlc = 0.030273590906016633\n",
    "#ex7_Slc = -0.006008375621810446\n",
    "#ex7_x = [1,1,1,1,1,1]\n",
    "#ex7_y = [0,0,0,0,0,0]\n",
    "#print(two_step_g_acq(ex7_mu,ex7_Sig,ex7_mlc,ex7_Slc,ex7_x,ex7_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd822b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will need to be moved!!!\n",
    "#Example7a: multlvl_two_step_g_acq\n",
    "#rng = np.random.default_rng(100) \n",
    "#np.random.seed(100)\n",
    "#random.seed(100)\n",
    "\n",
    "#ex7a_mu = rng.uniform(low = -1.0, high = 1.0, size = 6)\n",
    "#print(ex7a_mu)\n",
    "#ex7a_Sig = sklearn.datasets.make_spd_matrix(6)#[[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1]]\n",
    "#ex7a_mlc = 0.030273590906016633\n",
    "#ex7a_Slc = -0.006008375621810446\n",
    "#ex7a_x = [1,1,1,1,1,1]\n",
    "#ex7a_y = [0,0,0,0,0,0]\n",
    "#ex7a_num_lvl_vec = [2,2,2,2,2,2]\n",
    "#print(multlvl_two_step_g_acq(ex7a_mu,ex7a_Sig,ex7a_mlc,ex7a_Slc,ex7a_x,ex7a_y,ex7a_num_lvl_vec))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
